diff --git a/bench/dlrm_s_criteo_kaggle.sh b/bench/dlrm_s_criteo_kaggle.sh
index 867d8c0..17ced9e 100755
--- a/bench/dlrm_s_criteo_kaggle.sh
+++ b/bench/dlrm_s_criteo_kaggle.sh
@@ -9,6 +9,14 @@
 #check if extra argument is passed to the test
 if [[ $# == 1 ]]; then
     dlrm_extra_option=$1
+elif [[ $# == 2 ]]; then
+    dlrm_extra_option="$1 $2"
+elif [[ $# == 3 ]]; then
+    dlrm_extra_option="$1 $2 $3"
+elif [[ $# == 4 ]]; then
+    dlrm_extra_option="$1 $2 $3 $4"
+elif [[ $# == 5 ]]; then
+    dlrm_extra_option="$1 $2 $3 $4 $5"
 else
     dlrm_extra_option=""
 fi
@@ -21,12 +29,12 @@ echo "run pytorch ..."
 # WARNING: the following parameters will be set based on the data set
 # --arch-embedding-size=... (sparse feature sizes)
 # --arch-mlp-bot=... (the input to the first layer of bottom mlp)
-$dlrm_pt_bin --arch-sparse-feature-size=16 --arch-mlp-bot="13-512-256-64-16" --arch-mlp-top="512-256-1" --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=1024 --print-time --test-mini-batch-size=16384 --test-num-workers=16 $dlrm_extra_option 2>&1 | tee run_kaggle_pt.log
+$dlrm_pt_bin --arch-sparse-feature-size=16 --arch-mlp-bot="13-512-256-64-16" --arch-mlp-top="512-256-1" --data-generation=dataset --data-set=kaggle --raw-data-file=dlrm_kaggle/train.txt --processed-data-file=dlrm_kaggle/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=1024 --print-time --test-mini-batch-size=16384 --test-num-workers=16 --inference-only $dlrm_extra_option 2>&1 | tee run_kaggle_pt.log
 
-echo "run caffe2 ..."
+# echo "run caffe2 ..."
 # WARNING: the following parameters will be set based on the data set
 # --arch-embedding-size=... (sparse feature sizes)
 # --arch-mlp-bot=... (the input to the first layer of bottom mlp)
-$dlrm_c2_bin --arch-sparse-feature-size=16 --arch-mlp-bot="13-512-256-64-16" --arch-mlp-top="512-256-1" --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=1024 --print-time $dlrm_extra_option 2>&1 | tee run_kaggle_c2.log
+# $dlrm_c2_bin --arch-sparse-feature-size=16 --arch-mlp-bot="13-512-256-64-16" --arch-mlp-top="512-256-1" --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=1024 --print-time $dlrm_extra_option 2>&1 | tee run_kaggle_c2.log
 
 echo "done"
diff --git a/dlrm_s_pytorch.py b/dlrm_s_pytorch.py
index 5984238..be00919 100644
--- a/dlrm_s_pytorch.py
+++ b/dlrm_s_pytorch.py
@@ -61,7 +61,7 @@ import datetime
 import json
 import sys
 import time
-
+import os
 # onnx
 # The onnx import causes deprecation warnings every time workers
 # are spawned during testing. So, we filter out those warnings.
@@ -759,34 +759,64 @@ def inference(
 ):
     test_accu = 0
     test_samp = 0
+    t = 0
 
     if args.mlperf_logging:
         scores = []
         targets = []
 
+    if args.ipex:
+        if args.precision=="bfloat16":
+            conf = ipex.AmpConf(torch.bfloat16)
+            print("running bf16 evalation step\n")
+        else:
+            conf = ipex.AmpConf(None)
+            print("running fp32 evalation step\n")
+    
     for i, testBatch in enumerate(test_ld):
         # early exit if nbatches was set by the user and was exceeded
         if nbatches > 0 and i >= nbatches:
             break
 
-        X_test, lS_o_test, lS_i_test, T_test, W_test, CBPP_test = unpack_batch(
-            testBatch
-        )
+        X_test, lS_o_test, lS_i_test, T_test, W_test, CBPP_test = unpack_batch(testBatch)
+
+        if args.channels_last:
+            oob_X_test = X_test
+            oob_X_test = oob_X_test.to(memory_format=torch.channels_last)
+            X_test = oob_X_test
+            print("----Use channels last format.")
 
         # Skip the batch if batch size not multiple of total ranks
         if ext_dist.my_size > 1 and X_test.size(0) % ext_dist.my_size != 0:
             print("Warning: Skiping the batch %d with size %d" % (i, X_test.size(0)))
             continue
 
+        start = time.time()
         # forward pass
-        Z_test = dlrm_wrap(
-            X_test,
-            lS_o_test,
-            lS_i_test,
-            use_gpu,
-            device,
-            ndevices=ndevices,
-        )
+        if args.ipex:
+            with ipex.AutoMixPrecision(conf, running_mode="inference"):
+                if args.profile:
+                    with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU]) as prof:
+                        Z_test = dlrm_wrap(X_test,lS_o_test,lS_i_test,use_gpu,device,ndevices=ndevices,)
+                else:
+                    Z_test = dlrm_wrap(X_test,lS_o_test,lS_i_test,use_gpu,device,ndevices=ndevices,)
+        else:
+            if args.profile:
+                with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU]) as prof:
+                    if args.precision == 'bfloat16':
+                        with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                            Z_test = dlrm_wrap(X_test,lS_o_test,lS_i_test,use_gpu,device,ndevices=ndevices,)
+                    else:
+                        Z_test = dlrm_wrap(X_test,lS_o_test,lS_i_test,use_gpu,device,ndevices=ndevices,)
+            else:
+                if args.precision == 'bfloat16':
+                    with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                        Z_test = dlrm_wrap(X_test,lS_o_test,lS_i_test,use_gpu,device,ndevices=ndevices,)
+                else:
+                    Z_test = dlrm_wrap(X_test,lS_o_test,lS_i_test,use_gpu,device,ndevices=ndevices,)
+
+        end = time.time()
+        t += end - start
         ### gather the distributed results on each rank ###
         # For some reason it requires explicit sync before all_gather call if
         # tensor is on GPU memory
@@ -812,6 +842,18 @@ def inference(
 
                 test_accu += A_test
                 test_samp += mbs_test
+    #
+    if args.profile:
+        import pathlib
+        timeline_dir = str(pathlib.Path.cwd()) + '/timeline/'
+        if not os.path.exists(timeline_dir):
+            os.makedirs(timeline_dir)
+        timeline_file = timeline_dir + 'timeline-' + str(torch.backends.quantized.engine) + '-' + \
+                    "dlrm" + str(i) + '-' + str(os.getpid()) + '.json'
+        print(timeline_file)
+        prof.export_chrome_trace(timeline_file)
+        # table_res = prof.key_averages().table(sort_by="cpu_time_total")
+        # self.save_profile_result(timeline_dir + torch.backends.quantized.engine + "_result_average.xlsx", table_res)
 
     if args.mlperf_logging:
         with record_function("DLRM mlperf sklearn metrics compute"):
@@ -887,6 +929,7 @@ def inference(
             ),
             flush=True,
         )
+        print('Throughput is: %f its/s' % (len(test_ld) * 0.1 / t))
     return model_metrics_dict, is_best
 
 
@@ -911,6 +954,7 @@ def run():
     # embedding table options
     parser.add_argument("--md-flag", action="store_true", default=False)
     parser.add_argument("--md-threshold", type=int, default=200)
+    parser.add_argument("--arch", type=str, default="")
     parser.add_argument("--md-temperature", type=float, default=0.3)
     parser.add_argument("--md-round-dims", action="store_true", default=False)
     parser.add_argument("--qr-flag", action="store_true", default=False)
@@ -1007,6 +1051,16 @@ def run():
     parser.add_argument("--lr-num-warmup-steps", type=int, default=0)
     parser.add_argument("--lr-decay-start-step", type=int, default=0)
     parser.add_argument("--lr-num-decay-steps", type=int, default=0)
+    parser.add_argument('--ipex', action='store_true', default=False,
+                    help='use intel pytorch extension')
+    parser.add_argument('--channels_last', type=int, default=0,
+                        help='NHWC')
+    parser.add_argument('--precision', type=str, default="float32",
+                        help='precision, float32, bfloat16')
+    parser.add_argument('--jit', action='store_true', default=False,
+                        help='enable ipex jit fusionpath')
+    parser.add_argument('--profile', action='store_true', default=False,                        
+                        help='Trigger profile on current topology.')
 
     global args
     global nbatches
@@ -1063,6 +1117,10 @@ def run():
             ngpus = torch.cuda.device_count()
             device = torch.device("cuda", 0)
         print("Using {} GPU(s)...".format(ngpus))
+    elif args.ipex:
+        import intel_pytorch_extension as ipex
+        device = ipex.DEVICE
+        print("Using IPEX...")
     else:
         device = torch.device("cpu")
         print("Using CPU...")
@@ -1277,11 +1335,15 @@ def run():
             print(param.detach().cpu().numpy())
         # print(dlrm)
 
+    dlrm = dlrm.to(device)
+    if args.jit:
+        print("running jit fusion path\n")
+        # dlrm = torch.jit.script(dlrm.eval())
+        
     if use_gpu:
         # Custom Model-Data Parallel
         # the mlps are replicated and use data parallelism, while
         # the embeddings are distributed and use model parallelism
-        dlrm = dlrm.to(device)  # .cuda()
         if dlrm.ndevices > 1:
             dlrm.emb_l, dlrm.v_W_l = dlrm.create_emb(
                 m_spa, ln_emb, args.weighted_pooling
@@ -1479,9 +1541,282 @@ def run():
     writer = SummaryWriter(tb_file)
 
     ext_dist.barrier()
-    with torch.autograd.profiler.profile(
-        args.enable_profiling, use_gpu, record_shapes=True
-    ) as prof:
+    if args.enable_profiling:
+        with torch.autograd.profiler.profile(args.enable_profiling, use_gpu, record_shapes=True) as prof:
+            if not args.inference_only:
+                k = 0
+                total_time_begin = 0
+                while k < args.nepochs:
+                    if args.mlperf_logging:
+                        mlperf_logger.barrier()
+                        mlperf_logger.log_start(
+                            key=mlperf_logger.constants.BLOCK_START,
+                            metadata={
+                                mlperf_logger.constants.FIRST_EPOCH_NUM: (k + 1),
+                                mlperf_logger.constants.EPOCH_COUNT: 1,
+                            },
+                        )
+                        mlperf_logger.barrier()
+                        mlperf_logger.log_start(
+                            key=mlperf_logger.constants.EPOCH_START,
+                            metadata={mlperf_logger.constants.EPOCH_NUM: (k + 1)},
+                        )
+    
+                    if k < skip_upto_epoch:
+                        continue
+    
+                    if args.mlperf_logging:
+                        previous_iteration_time = None
+    
+                    for j, inputBatch in enumerate(train_ld):
+                        if j == 0 and args.save_onnx:
+                            X_onnx, lS_o_onnx, lS_i_onnx, _, _, _ = unpack_batch(inputBatch)
+    
+                        if j < skip_upto_batch:
+                            continue
+    
+                        X, lS_o, lS_i, T, W, CBPP = unpack_batch(inputBatch)
+    
+                        if args.mlperf_logging:
+                            current_time = time_wrap(use_gpu)
+                            if previous_iteration_time:
+                                iteration_time = current_time - previous_iteration_time
+                            else:
+                                iteration_time = 0
+                            previous_iteration_time = current_time
+                        else:
+                            t1 = time_wrap(use_gpu)
+    
+                        # early exit if nbatches was set by the user and has been exceeded
+                        if nbatches > 0 and j >= nbatches:
+                            break
+    
+                        # Skip the batch if batch size not multiple of total ranks
+                        if ext_dist.my_size > 1 and X.size(0) % ext_dist.my_size != 0:
+                            print(
+                                "Warning: Skiping the batch %d with size %d"
+                                % (j, X.size(0))
+                            )
+                            continue
+    
+                        mbs = T.shape[0]  # = args.mini_batch_size except maybe for last
+    
+                        # forward pass
+                        Z = dlrm_wrap(
+                            X,
+                            lS_o,
+                            lS_i,
+                            use_gpu,
+                            device,
+                            ndevices=ndevices,
+                        )
+    
+                        if ext_dist.my_size > 1:
+                            T = T[ext_dist.get_my_slice(mbs)]
+                            W = W[ext_dist.get_my_slice(mbs)]
+    
+                        # loss
+                        E = loss_fn_wrap(Z, T, use_gpu, device)
+    
+                        # compute loss and accuracy
+                        L = E.detach().cpu().numpy()  # numpy array
+                        # training accuracy is not disabled
+                        # S = Z.detach().cpu().numpy()  # numpy array
+                        # T = T.detach().cpu().numpy()  # numpy array
+    
+                        # # print("res: ", S)
+    
+                        # # print("j, train: BCE, shifted_BCE ", j, L, L_shifted)
+    
+                        # mbs = T.shape[0]  # = args.mini_batch_size except maybe for last
+                        # A = np.sum((np.round(S, 0) == T).astype(np.uint8))
+                        # A_shifted = np.sum((np.round(S_shifted, 0) == T).astype(np.uint8))
+    
+                        with record_function("DLRM backward"):
+                            # scaled error gradient propagation
+                            # (where we do not accumulate gradients across mini-batches)
+                            if (args.mlperf_logging and (j + 1) % args.mlperf_grad_accum_iter == 0) or not args.mlperf_logging:
+                                optimizer.zero_grad()
+                            # backward pass
+                            E.backward()
+    
+                            # optimizer
+                            if (args.mlperf_logging and (j + 1) % args.mlperf_grad_accum_iter == 0) or not args.mlperf_logging:
+                                optimizer.step()
+                                lr_scheduler.step()
+    
+                        if args.mlperf_logging:
+                            total_time += iteration_time
+                        else:
+                            t2 = time_wrap(use_gpu)
+                            total_time += t2 - t1
+    
+                        total_loss += L * mbs
+                        total_iter += 1
+                        total_samp += mbs
+    
+                        should_print = ((j + 1) % args.print_freq == 0) or (
+                            j + 1 == nbatches
+                        )
+                        should_test = (
+                            (args.test_freq > 0)
+                            and (args.data_generation == "dataset")
+                            and (((j + 1) % args.test_freq == 0) or (j + 1 == nbatches))
+                        )
+    
+                        # print time, loss and accuracy
+                        if should_print or should_test:
+                            gT = 1000.0 * total_time / total_iter if args.print_time else -1
+                            total_time = 0
+    
+                            train_loss = total_loss / total_samp
+                            total_loss = 0
+    
+                            str_run_type = (
+                                "inference" if args.inference_only else "training"
+                            )
+    
+                            wall_time = ""
+                            if args.print_wall_time:
+                                wall_time = " ({})".format(time.strftime("%H:%M"))
+    
+                            print(
+                                "Finished {} it {}/{} of epoch {}, {:.2f} ms/it,".format(
+                                    str_run_type, j + 1, nbatches, k, gT
+                                )
+                                + " loss {:.6f}".format(train_loss)
+                                + wall_time,
+                                flush=True,
+                            )
+    
+                            log_iter = nbatches * k + j + 1
+                            writer.add_scalar("Train/Loss", train_loss, log_iter)
+    
+                            total_iter = 0
+                            total_samp = 0
+    
+                        # testing
+                        if should_test:
+                            epoch_num_float = (j + 1) / len(train_ld) + k + 1
+                            if args.mlperf_logging:
+                                mlperf_logger.barrier()
+                                mlperf_logger.log_start(
+                                    key=mlperf_logger.constants.EVAL_START,
+                                    metadata={
+                                        mlperf_logger.constants.EPOCH_NUM: epoch_num_float
+                                    },
+                                )
+    
+                            # don't measure training iter time in a test iteration
+                            if args.mlperf_logging:
+                                previous_iteration_time = None
+                            print(
+                                "Testing at - {}/{} of epoch {},".format(j + 1, nbatches, k)
+                            )
+                            model_metrics_dict, is_best = inference(
+                                args,
+                                dlrm,
+                                best_acc_test,
+                                best_auc_test,
+                                test_ld,
+                                device,
+                                use_gpu,
+                                log_iter,
+                            )
+    
+                            if (
+                                is_best
+                                and not (args.save_model == "")
+                                and not args.inference_only
+                            ):
+                                model_metrics_dict["epoch"] = k
+                                model_metrics_dict["iter"] = j + 1
+                                model_metrics_dict["train_loss"] = train_loss
+                                model_metrics_dict["total_loss"] = total_loss
+                                model_metrics_dict[
+                                    "opt_state_dict"
+                                ] = optimizer.state_dict()
+                                print("Saving model to {}".format(args.save_model))
+                                torch.save(model_metrics_dict, args.save_model)
+    
+                            if args.mlperf_logging:
+                                mlperf_logger.barrier()
+                                mlperf_logger.log_end(
+                                    key=mlperf_logger.constants.EVAL_STOP,
+                                    metadata={
+                                        mlperf_logger.constants.EPOCH_NUM: epoch_num_float
+                                    },
+                                )
+    
+                            # Uncomment the line below to print out the total time with overhead
+                            # print("Total test time for this group: {}" \
+                            # .format(time_wrap(use_gpu) - accum_test_time_begin))
+    
+                            if (
+                                args.mlperf_logging
+                                and (args.mlperf_acc_threshold > 0)
+                                and (best_acc_test > args.mlperf_acc_threshold)
+                            ):
+                                print(
+                                    "MLPerf testing accuracy threshold "
+                                    + str(args.mlperf_acc_threshold)
+                                    + " reached, stop training"
+                                )
+                                break
+    
+                            if (
+                                args.mlperf_logging
+                                and (args.mlperf_auc_threshold > 0)
+                                and (best_auc_test > args.mlperf_auc_threshold)
+                            ):
+                                print(
+                                    "MLPerf testing auc threshold "
+                                    + str(args.mlperf_auc_threshold)
+                                    + " reached, stop training"
+                                )
+                                if args.mlperf_logging:
+                                    mlperf_logger.barrier()
+                                    mlperf_logger.log_end(
+                                        key=mlperf_logger.constants.RUN_STOP,
+                                        metadata={
+                                            mlperf_logger.constants.STATUS: mlperf_logger.constants.SUCCESS
+                                        },
+                                    )
+                                break
+    
+                    if args.mlperf_logging:
+                        mlperf_logger.barrier()
+                        mlperf_logger.log_end(
+                            key=mlperf_logger.constants.EPOCH_STOP,
+                            metadata={mlperf_logger.constants.EPOCH_NUM: (k + 1)},
+                        )
+                        mlperf_logger.barrier()
+                        mlperf_logger.log_end(
+                            key=mlperf_logger.constants.BLOCK_STOP,
+                            metadata={mlperf_logger.constants.FIRST_EPOCH_NUM: (k + 1)},
+                        )
+                    k += 1  # nepochs
+                if args.mlperf_logging and best_auc_test <= args.mlperf_auc_threshold:
+                    mlperf_logger.barrier()
+                    mlperf_logger.log_end(
+                        key=mlperf_logger.constants.RUN_STOP,
+                        metadata={
+                            mlperf_logger.constants.STATUS: mlperf_logger.constants.ABORTED
+                        },
+                    )
+            else:
+                print("Testing for inference only")
+                inference(
+                    args,
+                    dlrm,
+                    best_acc_test,
+                    best_auc_test,
+                    test_ld,
+                    device,
+                    use_gpu,
+                )
+
+    else:
         if not args.inference_only:
             k = 0
             total_time_begin = 0
@@ -1867,6 +2202,24 @@ def run():
         onnx.checker.check_model(dlrm_pytorch_onnx)
     total_time_end = time_wrap(use_gpu)
 
+def save_profile_result(filename, table):
+    import xlsxwriter
+    workbook = xlsxwriter.Workbook(filename)
+    worksheet = workbook.add_worksheet()
+    keys = ["Name", "Self CPU total %", "Self CPU total", "CPU total %" , "CPU total", \
+            "CPU time avg", "Number of Calls"]
+    for j in range(len(keys)):
+        worksheet.write(0, j, keys[j])
+
+    lines = table.split("\n")
+    for i in range(3, len(lines)-4):
+        words = lines[i].split(" ")
+        j = 0
+        for word in words:
+            if not word == "":
+                worksheet.write(i-2, j, word)
+                j += 1
+    workbook.close()
 
 if __name__ == "__main__":
     run()
