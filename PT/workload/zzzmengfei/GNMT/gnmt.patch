diff --git a/rnn_translator/pytorch/seq2seq/data/config.py b/rnn_translator/pytorch/seq2seq/data/config.py
index 0582e04..bde99bc 100644
--- a/rnn_translator/pytorch/seq2seq/data/config.py
+++ b/rnn_translator/pytorch/seq2seq/data/config.py
@@ -12,8 +12,8 @@ VOCAB_FNAME = 'vocab.bpe.32000'
 
 # paths to source and target training files, relative to the data directory, it
 # should point to BPE-encoded files, generated by subword-nmt/apply_bpe.py
-SRC_TRAIN_FNAME = 'train.tok.clean.bpe.32000.en'
-TGT_TRAIN_FNAME = 'train.tok.clean.bpe.32000.de'
+SRC_TRAIN_FNAME = 'newstest2013.tok.clean.bpe.32000.en'
+TGT_TRAIN_FNAME = 'newstest2013.tok.clean.bpe.32000.de'
 
 # paths to source and target validation files, relative to the data directory,
 # it should point to BPE-encoded files, generated by subword-nmt/apply_bpe.py
diff --git a/rnn_translator/pytorch/seq2seq/data/dataset.py b/rnn_translator/pytorch/seq2seq/data/dataset.py
index 5d63884..37c75fb 100644
--- a/rnn_translator/pytorch/seq2seq/data/dataset.py
+++ b/rnn_translator/pytorch/seq2seq/data/dataset.py
@@ -165,7 +165,7 @@ class TextDataset(Dataset):
         """
         logging.info(f'Processing data from {fname}')
         data = []
-        with open(fname) as dfile:
+        with open(fname, encoding='utf-8') as dfile:
             for idx, line in enumerate(dfile):
                 if max_size and idx == max_size:
                     break
@@ -338,7 +338,7 @@ class LazyParallelDataset(TextDataset):
         """
         logging.info(f'Processing data from {fname}')
         data = []
-        with open(fname) as dfile:
+        with open(fname, encoding='utf-8') as dfile:
             for idx, line in enumerate(dfile):
                 if max_size and idx == max_size:
                     break
diff --git a/rnn_translator/pytorch/seq2seq/data/tokenizer.py b/rnn_translator/pytorch/seq2seq/data/tokenizer.py
index b2d9549..a440c94 100644
--- a/rnn_translator/pytorch/seq2seq/data/tokenizer.py
+++ b/rnn_translator/pytorch/seq2seq/data/tokenizer.py
@@ -24,7 +24,7 @@ class Tokenizer:
             vocab = [config.PAD_TOKEN, config.UNK_TOKEN,
                      config.BOS_TOKEN, config.EOS_TOKEN]
 
-            with open(vocab_fname) as vfile:
+            with open(vocab_fname, encoding='utf-8') as vfile:
                 for line in vfile:
                     vocab.append(line.strip())
 
diff --git a/rnn_translator/pytorch/seq2seq/inference/inference.py b/rnn_translator/pytorch/seq2seq/inference/inference.py
index 5ec3a4b..62b2ef3 100644
--- a/rnn_translator/pytorch/seq2seq/inference/inference.py
+++ b/rnn_translator/pytorch/seq2seq/inference/inference.py
@@ -117,7 +117,7 @@ class Translator:
         self.model.eval()
         torch.cuda.empty_cache()
 
-        output = self.evaluate(epoch, iteration, summary)
+        output, perf = self.evaluate(epoch, iteration, summary)
         output = output[:len(self.loader.dataset)]
         output = self.loader.dataset.unsort(output)
 
@@ -142,7 +142,7 @@ class Translator:
             dist.broadcast(break_training, 0)
             dist.broadcast(test_bleu, 0)
 
-        return test_bleu[0].item(), break_training[0].item()
+        return test_bleu[0].item(), break_training[0].item(), perf
 
     def evaluate(self, epoch, iteration, summary):
         """
@@ -251,8 +251,9 @@ class Translator:
             log += f'Total decoder iterations: {int(iterations.sum)}'
             log = ''.join(log)
             logging.info(log)
+        perf = tot_tok_per_sec.avg
 
-        return output
+        return output, perf
 
     def run_detokenizer(self, eval_path):
         """
diff --git a/rnn_translator/pytorch/seq2seq/train/trainer.py b/rnn_translator/pytorch/seq2seq/train/trainer.py
index c7a2b75..42b3a46 100644
--- a/rnn_translator/pytorch/seq2seq/train/trainer.py
+++ b/rnn_translator/pytorch/seq2seq/train/trainer.py
@@ -7,7 +7,7 @@ import numpy as np
 import torch
 import torch.optim
 import torch.utils.data
-from apex.parallel import DistributedDataParallel as DDP
+# from apex.parallel import DistributedDataParallel as DDP
 from mlperf_compliance import mlperf_log
 
 from seq2seq.train.fp_optimizers import Fp16Optimizer
@@ -98,8 +98,8 @@ class Seq2SeqTrainer:
         if math == 'fp16':
             self.model = self.model.half()
 
-        if distributed:
-            self.model = DDP(self.model)
+        # if distributed:
+        #     self.model = DDP(self.model)
 
         if math == 'fp16':
             self.fp_optimizer = Fp16Optimizer(self.model, grad_clip)
@@ -172,7 +172,7 @@ class Seq2SeqTrainer:
 
         return loss_per_token, loss_per_sentence, num_toks
 
-    def feed_data(self, data_loader, training=True):
+    def feed_data(self, data_loader, args, training=True):
         """
         Runs training or validation on batches from data_loader.
 
@@ -200,6 +200,8 @@ class Seq2SeqTrainer:
 
         end = time.time()
         for i, (src, tgt) in enumerate(data_loader):
+            if args.val_num_iters != 0 and i == args.val_num_iters:
+                break
             self.save_counter += 1
             # measure data loading time
             data_time.update(time.time() - end)
@@ -217,12 +219,13 @@ class Seq2SeqTrainer:
             losses_per_sentence.update(loss_per_sentence, batch_size)
 
             # measure elapsed time
-            elapsed = time.time() - end
-            batch_time.update(elapsed)
-            src_tok_time.update(num_toks['src'] / elapsed)
-            tgt_tok_time.update(num_toks['tgt'] / elapsed)
-            tot_num_toks = num_toks['tgt'] + num_toks['src']
-            tot_tok_time.update(tot_num_toks / elapsed)
+            if i >= args.val_num_warmup:
+                elapsed = time.time() - end
+                batch_time.update(elapsed)
+                src_tok_time.update(num_toks['src'] / elapsed)
+                tgt_tok_time.update(num_toks['tgt'] / elapsed)
+                tot_num_toks = num_toks['tgt'] + num_toks['src']
+                tot_tok_time.update(tot_num_toks / elapsed)
             self.loss = losses_per_token.avg
 
             if training and i in eval_iters:
@@ -316,7 +319,7 @@ class Seq2SeqTrainer:
         torch.cuda.empty_cache()
         return output
 
-    def evaluate(self, data_loader):
+    def evaluate(self, data_loader, args):
         """
         Sets model in eval mode, disables gradients, preallocates memory and
         runs validation on data provided by data_loader.
@@ -327,7 +330,7 @@ class Seq2SeqTrainer:
         self.model.eval()
         torch.cuda.empty_cache()
         self.preallocate(data_loader, training=False)
-        output = self.feed_data(data_loader, training=False)
+        output = self.feed_data(data_loader, args, training=False)
         self.model.zero_grad()
         torch.cuda.empty_cache()
         return output
diff --git a/rnn_translator/pytorch/train.py b/rnn_translator/pytorch/train.py
index a4906c9..3fd3a1f 100644
--- a/rnn_translator/pytorch/train.py
+++ b/rnn_translator/pytorch/train.py
@@ -22,6 +22,10 @@ from seq2seq.inference.inference import Translator
 from seq2seq.models.gnmt import GNMT
 from seq2seq.train.smoothing import LabelSmoothing
 from seq2seq.utils import gnmt_print
+try:
+    import intel_pytorch_extension as ipex
+except:
+    pass
 
 
 def parse_args():
@@ -44,7 +48,7 @@ def parse_args():
 
     # dataset
     dataset = parser.add_argument_group('dataset setup')
-    dataset.add_argument('--dataset-dir', default='data/wmt16_de_en',
+    dataset.add_argument('--dataset-dir', default='../data',
                          help='path to the directory with training/test data')
     dataset.add_argument('--max-size', default=None, type=int,
                          help='use at most MAX_SIZE elements from training \
@@ -83,7 +87,7 @@ def parse_args():
     general = parser.add_argument_group('general setup')
     general.add_argument('--math', default='fp32', choices=['fp16', 'fp32'],
                          help='arithmetic type')
-    general.add_argument('--seed', default=None, type=int,
+    general.add_argument('--seed', default=1, type=int,
                          help='master seed for random number generators, if \
                          "seed" is undefined then the master seed will be \
                          sampled from random.SystemRandom()')
@@ -92,14 +96,14 @@ def parse_args():
                     help='run validation and test after every epoch')
     exclusive_group(group=general, name='env', default=False,
                     help='print info about execution env')
-    exclusive_group(group=general, name='cuda', default=True,
+    exclusive_group(group=general, name='cuda', default=False,
                     help='enables cuda')
-    exclusive_group(group=general, name='cudnn', default=True,
+    exclusive_group(group=general, name='cudnn', default=False,
                     help='enables cudnn')
 
     # training
     training = parser.add_argument_group('training setup')
-    training.add_argument('--train-batch-size', default=128, type=int,
+    training.add_argument('--train-batch-size', default=64, type=int,
                           help='training batch size per worker')
     training.add_argument('--train-global-batch-size', default=None, type=int,
                           help='global training batch size, this argument \
@@ -114,7 +118,7 @@ def parse_args():
                           help='training iter size, training loop will \
                           accumulate gradients over N iterations and execute \
                           optimizer every N steps')
-    training.add_argument('--epochs', default=8, type=int,
+    training.add_argument('--epochs', default=1, type=int,
                           help='max number of training epochs')
 
     training.add_argument('--grad-clip', default=5.0, type=float,
@@ -165,6 +169,10 @@ def parse_args():
     val = parser.add_argument_group('validation setup')
     val.add_argument('--val-batch-size', default=64, type=int,
                      help='batch size for validation')
+    val.add_argument('--val-num-iters', default=0, type=int,
+                     help='iters for validation')
+    val.add_argument('--val-num-warmup', default=10, type=int,
+                     help='warmup for validation')
     val.add_argument('--max-length-val', default=125, type=int,
                      help='maximum sequence length for validation \
                      (including special BOS and EOS tokens)')
@@ -173,10 +181,12 @@ def parse_args():
                      (including special BOS and EOS tokens)')
     val.add_argument('--val-loader-workers', default=0, type=int,
                      help='number of workers for validation data loading')
+    val.add_argument('--channels-last', default=0, type=int,
+                     help='channels last format')
 
     # test
     test = parser.add_argument_group('test setup')
-    test.add_argument('--test-batch-size', default=128, type=int,
+    test.add_argument('--test-batch-size', default=64, type=int,
                       help='batch size for test')
     test.add_argument('--max-length-test', default=150, type=int,
                       help='maximum sequence length for test \
@@ -227,6 +237,19 @@ def parse_args():
     distributed.add_argument('--local_rank', default=0, type=int,
                              help='local rank of the process, do not set!')
 
+    # ipex
+    ipe = parser.add_argument_group('ipex setup')
+    ipe.add_argument('--ipex', action='store_true', default=False,
+                    help='use intel pytorch extension')
+    ipe.add_argument('--precision', type=str, default="float32",
+                    help='precision, float32, bfloat16')
+    ipe.add_argument('--jit', action='store_true', default=False,
+                    help='enable ipex jit fusionpath')
+    ipe.add_argument('--inference', action='store_true', default=False,
+                    help='Inference only')
+    ipe.add_argument('--profile', action='store_true', default=False,
+                    help='profile')
+
     args = parser.parse_args()
 
     args.warmup_steps = literal_eval(args.warmup_steps)
@@ -236,12 +259,12 @@ def parse_args():
     return args
 
 
-def build_criterion(vocab_size, padding_idx, smoothing):
+def build_criterion(vocab_size, padding_idx, smoothing, device):
     if smoothing == 0.:
         logging.info(f'Building CrossEntropyLoss')
         loss_weight = torch.ones(vocab_size)
         loss_weight[padding_idx] = 0
-        criterion = nn.CrossEntropyLoss(weight=loss_weight, size_average=False)
+        criterion = nn.CrossEntropyLoss(weight=loss_weight, size_average=False).to(device)
         gnmt_print(key=mlperf_log.MODEL_HP_LOSS_FN,
                    value='Cross Entropy', sync=False)
     else:
@@ -263,7 +286,14 @@ def main():
     mlperf_log.LOGGER.propagate = False
 
     args = parse_args()
-    device = utils.set_device(args.cuda, args.local_rank)
+    if args.cuda:
+        torch.cuda.set_device(args.local_rank)
+        device = torch.device('cuda')
+    elif args.ipex:
+        device = ipex.DEVICE
+    else:
+        device = torch.device('cpu')
+
     distributed = utils.init_distributed(args.cuda)
     gnmt_print(key=mlperf_log.RUN_START, sync=True)
     args.rank = utils.get_rank()
@@ -360,7 +390,17 @@ def main():
     batch_first = model.batch_first
 
     # define loss function (criterion) and optimizer
-    criterion = build_criterion(vocab_size, config.PAD, args.smoothing)
+    criterion = build_criterion(vocab_size, config.PAD, args.smoothing, device)
+
+    model = model.to(device)
+    if args.jit:
+        print("running jit fusion path\n")
+        # model = torch.jit.script(model)
+    if args.channels_last:
+        oob_model = model
+        oob_model = oob_model.to(memory_format=torch.channels_last)
+        model = oob_model
+        print("---- Use channels last format.")
 
     opt_config = {'optimizer': args.optimizer, 'lr': args.lr}
     opt_config.update(literal_eval(args.optimizer_extra))
@@ -410,7 +450,7 @@ def main():
 
     translator = Translator(model=model,
                             tokenizer=tokenizer,
-                            loader=test_loader,
+                            loader=val_loader,
                             beam_size=args.beam_size,
                             max_seq_len=args.max_length_test,
                             len_norm_factor=args.len_norm_factor,
@@ -464,7 +504,17 @@ def main():
     break_training = False
     test_bleu = None
     gnmt_print(key=mlperf_log.TRAIN_LOOP, sync=True)
-    for epoch in range(args.start_epoch, args.epochs):
+    t = 0
+
+    if args.ipex:
+        if args.precision=="bfloat16":
+            conf = ipex.AmpConf(torch.bfloat16)
+            print("running bf16 evalation step\n")
+        else:
+            conf = ipex.AmpConf(None)
+            print("running fp32 evalation step\n")
+
+    for index, epoch in enumerate(range(args.start_epoch, args.epochs)):
         logging.info(f'Starting epoch {epoch}')
         gnmt_print(key=mlperf_log.TRAIN_EPOCH,
                    value=epoch, sync=True)
@@ -472,12 +522,51 @@ def main():
         train_loader.sampler.set_epoch(epoch)
 
         trainer.epoch = epoch
+
+        if args.inference:
+            if args.ipex:
+                if args.profile:
+                    with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU]) as prof:
+                        with ipex.AutoMixPrecision(conf, running_mode="inference"):
+                            if args.precision =='bfloat16':
+                                with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                                    val_loss, val_perf = trainer.evaluate(val_loader, args)
+                            else:
+                                val_loss, val_perf = trainer.evaluate(val_loader, args)
+                else:
+                    with ipex.AutoMixPrecision(conf, running_mode="inference"):
+                        if args.precision =='bfloat16':
+                            with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                                val_loss, val_perf = trainer.evaluate(val_loader, args)
+                        else:
+                            val_loss, val_perf = trainer.evaluate(val_loader, args)
+            else:
+                if args.profile:
+                    with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU]) as prof:
+                        if args.precision =='bfloat16':
+                            with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                                val_loss, val_perf = trainer.evaluate(val_loader, args)
+                        else:
+                            val_loss, val_perf = trainer.evaluate(val_loader, args)
+                else:
+                    if args.precision =='bfloat16':
+                        with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                            val_loss, val_perf = trainer.evaluate(val_loader, args)
+                    else:
+                        val_loss, val_perf = trainer.evaluate(val_loader, args)
+            perf_log = []
+            perf_log += [f'Performance: Epoch: {epoch}']
+            perf_log += [f'Inference: {val_perf:.0f} Tok/s']
+            logging.info('\t'.join(perf_log))
+            logging.info(f'Finished epoch {epoch}')
+            continue
+
         train_loss, train_perf = trainer.optimize(train_loader)
 
         # evaluate on validation set
         if args.eval:
             logging.info(f'Running validation on dev set')
-            val_loss, val_perf = trainer.evaluate(val_loader)
+            val_loss, val_perf = trainer.evaluate(val_loader, args)
 
             # remember best prec@1 and save checkpoint
             gnmt_print(key=mlperf_log.TRAIN_CHECKPOINT, sync=False)
@@ -488,8 +577,14 @@ def main():
 
         if args.eval:
             gnmt_print(key=mlperf_log.EVAL_START, value=epoch, sync=True)
-            test_bleu, break_training = translator.run(calc_bleu=True,
-                                                       epoch=epoch)
+            if args.ipex:
+                with ipex.AutoMixPrecision(conf, running_mode="inference"):
+                    test_bleu, break_training, inf_perf = translator.run(calc_bleu=False,
+                                                           epoch=epoch)
+            else:
+                test_bleu, break_training, inf_perf = translator.run(calc_bleu=False,
+                                                           epoch=epoch)
+
             gnmt_print(key=mlperf_log.EVAL_ACCURACY,
                        value={"epoch": epoch, "value": round(test_bleu, 2)},
                        sync=False)
@@ -509,6 +604,7 @@ def main():
         perf_log += [f'Training: {train_perf:.0f} Tok/s']
         if args.eval:
             perf_log += [f'Validation: {val_perf:.0f} Tok/s']
+            perf_log += [f'Throughput: {inf_perf:.0f} Tok/s']
 
         if args.rank == 0:
             logging.info('\t'.join(acc_log))
@@ -518,10 +614,42 @@ def main():
         if break_training:
             break
 
-    gnmt_print(key=mlperf_log.RUN_STOP,
-               value={"success": bool(break_training)}, sync=True)
+    #
+    if args.profile:
+        import pathlib
+        timeline_dir = str(pathlib.Path.cwd()) + '/timeline/'
+        if not os.path.exists(timeline_dir):
+            os.makedirs(timeline_dir)
+        timeline_file = timeline_dir + 'timeline-' + str(torch.backends.quantized.engine) + '-' + \
+                    'GNMT-' + str(index + 1) + '-' + str(os.getpid()) + '.json'
+        prof.export_chrome_trace(timeline_file)
+        # table_res = prof.key_averages().table(sort_by="cpu_time_total")
+        # save_profile_result(timeline_dir + torch.backends.quantized.engine + "_result_average.xlsx", table_res)
+
+    # gnmt_print(key=mlperf_log.RUN_STOP,
+    #            value={"success": bool(break_training)}, sync=True)
     gnmt_print(key=mlperf_log.RUN_FINAL, sync=False)
 
 
+def save_profile_result(filename, table):
+    import xlsxwriter
+    workbook = xlsxwriter.Workbook(filename)
+    worksheet = workbook.add_worksheet()
+    keys = ["Name", "Self CPU total %", "Self CPU total", "CPU total %" , "CPU total", \
+            "CPU time avg", "Number of Calls"]
+    for j in range(len(keys)):
+        worksheet.write(0, j, keys[j])
+
+    lines = table.split("\n")
+    for i in range(3, len(lines)-4):
+        words = lines[i].split(" ")
+        j = 0
+        for word in words:
+            if not word == "":
+                worksheet.write(i-2, j, word)
+                j += 1
+    workbook.close()
+
+
 if __name__ == '__main__':
     main()
