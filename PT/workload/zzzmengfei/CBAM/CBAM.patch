diff --git a/train_imagenet.py b/train_imagenet.py
index b48cb23..7a5c6fe 100644
--- a/train_imagenet.py
+++ b/train_imagenet.py
@@ -21,7 +21,7 @@ model_names = sorted(name for name in models.__dict__
     and callable(models.__dict__[name]))
 
 parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')
-parser.add_argument('data', metavar='DIR',
+parser.add_argument('--data', metavar='DIR', default="",
                     help='path to dataset')
 parser.add_argument('--arch', '-a', metavar='ARCH', default='resnet',
                     help='model architecture: ' +
@@ -53,15 +53,41 @@ parser.add_argument("--seed", type=int, default=1234, metavar='BS', help='input
 parser.add_argument("--prefix", type=str, required=True, metavar='PFX', help='prefix for logging & checkpoint saving')
 parser.add_argument('--evaluate', dest='evaluate', action='store_true', help='evaluation only')
 parser.add_argument('--att-type', type=str, choices=['BAM', 'CBAM'], default=None)
+parser.add_argument('--cuda', action='store_true', default=False,
+                    help='use cuda')
+parser.add_argument('--dummy', action='store_true', default=False,
+                    help='use dummy data')
+parser.add_argument('--ipex', action='store_true', default=False,
+                    help='use ipex')
+parser.add_argument('--jit', action='store_true', default=False,
+                    help='use ipex')
+parser.add_argument('--precision', default="float32",
+                        help='precision, "float32" or "bfloat16", default is "float32"')
+parser.add_argument('--warmup', type=int, default=5,
+                    help='number of warmup')
+parser.add_argument('--max_iters', type=int, default=10,
+                    help='max number of iterations to run')
+parser.add_argument('--profile', action='store_true', default=False,
+                    help='Trigger profile on current topology.')
+parser.add_argument('--channels_last', type=int, default=1, help='use channels last format')
 best_prec1 = 0
 
+args = parser.parse_args()
+if args.ipex:
+    import intel_pytorch_extension as ipex
+    print("Running with IPEX...")
+    if args.precision == "bfloat16":
+        # Automatically mix precision
+        ipex.enable_auto_mixed_precision(mixed_dtype=torch.bfloat16)
+        print('Running with bfloat16...')
+
 if not os.path.exists('./checkpoints'):
     os.mkdir('./checkpoints')
 
 def main():
     global args, best_prec1
     global viz, train_lot, test_lot
-    args = parser.parse_args()
+    # args = parser.parse_args()
     print ("args", args)
 
     torch.manual_seed(args.seed)
@@ -69,7 +95,7 @@ def main():
     random.seed(args.seed)
 
     # create model
-    if args.arch == "resnet":
+    if args.arch == "CBAM":
         model = ResidualNet( 'ImageNet', args.depth, 1000, args.att_type )
 
     # define loss function (criterion) and optimizer
@@ -78,11 +104,25 @@ def main():
     optimizer = torch.optim.SGD(model.parameters(), args.lr,
                             momentum=args.momentum,
                             weight_decay=args.weight_decay)
-    model = torch.nn.DataParallel(model, device_ids=list(range(args.ngpu)))
-    #model = torch.nn.DataParallel(model).cuda()
-    model = model.cuda()
-    print ("model")
-    print (model)
+
+    if args.ipex:
+        model = torch.nn.DataParallel(model)
+        model = model.to(ipex.DEVICE)
+        # if args.jit:
+        #     model = torch.jit.script(model)
+    elif args.cuda:
+        model = torch.nn.DataParallel(model, device_ids=list(range(args.ngpu)))
+        model = model.cuda()
+    else:
+        model = torch.nn.DataParallel(model)
+    #
+    if args.channels_last:
+        model = torch.nn.DataParallel(model)
+        model_oob = model
+        model_oob = model_oob.to(memory_format=torch.channels_last)
+        model = model_oob
+        print("---- Use channels last format.")
+    # print (model)
 
     # get the number of model parameters
     print('Number of model parameters: {}'.format(
@@ -103,26 +143,30 @@ def main():
         else:
             print("=> no checkpoint found at '{}'".format(args.resume))
 
+    if args.cuda:
+        cudnn.benchmark = True
+
+    if not args.dummy:
+        # Data loading code
+        traindir = os.path.join(args.data, 'train')
+        valdir = os.path.join(args.data, 'val')
+        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+        # import pdb
+        # pdb.set_trace()
+        val_loader = torch.utils.data.DataLoader(
+            datasets.ImageFolder(valdir, transforms.Compose([
+                    transforms.Scale(256),
+                    transforms.CenterCrop(224),
+                    transforms.ToTensor(),
+                    normalize,
+                    ])),
+                batch_size=args.batch_size, shuffle=False,
+               num_workers=args.workers, pin_memory=True)
+    else:
+        val_loader = None
 
-    cudnn.benchmark = True
-
-    # Data loading code
-    traindir = os.path.join(args.data, 'train')
-    valdir = os.path.join(args.data, 'val')
-    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
-                                 std=[0.229, 0.224, 0.225])
-
-    # import pdb
-    # pdb.set_trace()
-    val_loader = torch.utils.data.DataLoader(
-        datasets.ImageFolder(valdir, transforms.Compose([
-                transforms.Scale(256),
-                transforms.CenterCrop(224),
-                transforms.ToTensor(),
-                normalize,
-                ])),
-            batch_size=args.batch_size, shuffle=False,
-           num_workers=args.workers, pin_memory=True)
     if args.evaluate:
         validate(val_loader, model, criterion, 0)
         return
@@ -177,8 +221,9 @@ def train(train_loader, model, criterion, optimizer, epoch):
     for i, (input, target) in enumerate(train_loader):
         # measure data loading time
         data_time.update(time.time() - end)
-        
-        target = target.cuda(async=True)
+
+        # target = target.cuda(async=True)
+        target = target.cuda()
         input_var = torch.autograd.Variable(input)
         target_var = torch.autograd.Variable(target)
         
@@ -220,37 +265,113 @@ def validate(val_loader, model, criterion, epoch):
     # switch to evaluate mode
     model.eval()
 
-    end = time.time()
-    for i, (input, target) in enumerate(val_loader):
-        target = target.cuda(async=True)
+    if args.dummy:
+        input = torch.randn(args.batch_size, 3, 224, 224)
         input_var = torch.autograd.Variable(input, volatile=True)
-        target_var = torch.autograd.Variable(target, volatile=True)
-        
-        # compute output
-        output = model(input_var)
-        loss = criterion(output, target_var)
-        
-        # measure accuracy and record loss
-        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))
-        losses.update(loss.data[0], input.size(0))
-        top1.update(prec1[0], input.size(0))
-        top5.update(prec5[0], input.size(0))
-        
-        # measure elapsed time
-        batch_time.update(time.time() - end)
-        end = time.time()
-        
-        if i % args.print_freq == 0:
-            print('Test: [{0}/{1}]\t'
-                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
-                  'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
-                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\t'
-                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(
-                   i, len(val_loader), batch_time=batch_time, loss=losses,
-                   top1=top1, top5=top5))
-    
-    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'
+        if args.ipex:
+            input_var = input_var.to(ipex.DEVICE)
+            if args.jit:
+                model = torch.jit.trace(model, input_var)
+        elif args.cuda:
+            input_var = input_var.cuda()
+        if args.channels_last:
+            input_var_oob = input_var
+            input_var_oob = input_var_oob.to(memory_format=torch.channels_last)
+            input_var = input_var_oob
+        for i in range(args.max_iters):
+            # compute output
+            if i >= args.warmup:
+                end = time.time()
+            if args.profile:
+                with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU]) as prof:
+                    if args.precision == "bfloat16":
+                        with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                            output = model(input_var)
+                    else:
+                        output = model(input_var)
+            else:
+                if args.precision == "bfloat16":
+                    with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                        output = model(input_var)
+                else:
+                    output = model(input_var)
+
+            # measure elapsed time
+            if i >= args.warmup:
+                batch_time.update(time.time() - end)
+
+            if i % args.print_freq == 0:
+                print('Test: [{0}/{1}]\t'
+                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'.format(
+                       i, args.max_iters + args.warmup, batch_time=batch_time))
+
+    else:
+        for i, (input, target) in enumerate(val_loader):
+            if args.max_iters > 0 and i >= args.max_iters:
+                break
+            if i >= args.warmup:
+                end = time.time()
+            # target = target.cuda(async=True)
+            if args.ipex:
+                target = target.to(ipex.DEVICE)
+                input_var = torch.autograd.Variable(input.to(ipex.DEVICE), volatile=True)
+                target_var = torch.autograd.Variable(target, volatile=True)
+            elif args.cuda:
+                target = target.cuda()
+                input_var = torch.autograd.Variable(input, volatile=True)
+                target_var = torch.autograd.Variable(target, volatile=True)
+            else:
+                target = target.cuda()
+                input_var = torch.autograd.Variable(input, volatile=True)
+                target_var = torch.autograd.Variable(target, volatile=True)
+
+            # compute output
+            if args.profile:
+                with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU]) as prof:
+                    output = model(input_var)
+            else:
+                output = model(images)
+
+            #output = model(input_var)
+            loss = criterion(output, target_var)
+
+            # measure accuracy and record loss
+            prec1, prec5 = accuracy(output.data, target, topk=(1, 5))
+            losses.update(loss.data[0], input.size(0))
+            top1.update(prec1[0], input.size(0))
+            top5.update(prec5[0], input.size(0))
+
+            # measure elapsed time
+            if i >= args.warmup:
+                batch_time.update(time.time() - end)
+
+            if i % args.print_freq == 0:
+                print('Test: [{0}/{1}]\t'
+                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
+                      'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
+                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\t'
+                      'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(
+                       i, len(val_loader), batch_time=batch_time, loss=losses,
+                       top1=top1, top5=top5))
+            if i>= args.warmup:
+                break
+        print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'
             .format(top1=top1, top5=top5))
+    latency = batch_time.avg / args.batch_size * 1000
+    perf = args.batch_size / batch_time.avg
+    print('inference latency: %0.3f ms' % latency)
+    print('inference Throughput: %0.3f fps' % perf)
+
+    if args.profile:
+        import pathlib
+        timeline_dir = str(pathlib.Path.cwd()) + '/timeline/'
+        if not os.path.exists(timeline_dir):
+            os.makedirs(timeline_dir)
+        timeline_file = timeline_dir + 'timeline-' + str(torch.backends.quantized.engine) + '-' + \
+                    args.arch + '-' + str(i + 1) + '-' + str(os.getpid()) + '.json'
+        prof.export_chrome_trace(timeline_file)
+        # table_res = prof.key_averages().table(sort_by="cpu_time_total")
+        # save_profile_result(timeline_dir + torch.backends.quantized.engine + "_result_average.xlsx", table_res)
 
     return top1.avg
 
@@ -301,6 +422,24 @@ def accuracy(output, target, topk=(1,)):
         correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)
         res.append(correct_k.mul_(100.0 / batch_size))
     return res
+def save_profile_result(filename, table):
+    import xlsxwriter
+    workbook = xlsxwriter.Workbook(filename)
+    worksheet = workbook.add_worksheet()
+    keys = ["Name", "Self CPU total %", "Self CPU total", "CPU total %" , "CPU total", \
+            "CPU time avg", "Number of Calls"]
+    for j in range(len(keys)):
+        worksheet.write(0, j, keys[j])
+
+    lines = table.split("\n")
+    for i in range(3, len(lines)-4):
+        words = lines[i].split(" ")
+        j = 0
+        for word in words:
+            if not word == "":
+                worksheet.write(i-2, j, word)
+                j += 1
+    workbook.close()
 
 
 if __name__ == '__main__':
