diff --git a/main.py b/main.py
index 8509748..809f9b4 100644
--- a/main.py
+++ b/main.py
@@ -15,15 +15,20 @@ params['DataManagerParams'] = dict()
 params['ModelParams'] = dict()
 
 #  params of the algorithm
-params['ModelParams']['numcontrolpoints'] = 2  #？？？
+params['ModelParams']['numcontrolpoints'] = 2  # ？？？
 params['ModelParams']['sigma'] = 15
 params['ModelParams']['device'] = 0
 params['ModelParams']['snapshot'] = 0
-params['ModelParams']['dirTrain'] = os.path.join(basePath,'dataset/Train')  # dataset from PROMISE12: prostate MRI scans, training case00-44(https://promise12.grand-challenge.org/download/#)
-params['ModelParams']['dirTest'] = os.path.join(basePath,'dataset/Test') # dataset from PROMISE12: prostate MRI scans, training case45-49
-params['ModelParams']['dirInfer'] = os.path.join(basePath,'dataset/Infer') # dataset from PROMISE12: prostate MRI scans, 30 testing
-params['ModelParams']['dirResult'] = os.path.join(basePath,'results')  # where we need to save the results (relative to the base path)
-params['ModelParams']['dirSnapshots'] = os.path.join(basePath,'Models/MRI_cinque_snapshots/')  # where to save the models while training
+# dataset from PROMISE12: prostate MRI scans, training case00-44(https://promise12.grand-challenge.org/download/#)
+params['ModelParams']['dirTrain'] = os.path.join(basePath, 'dataset/Train')
+# dataset from PROMISE12: prostate MRI scans, training case45-49
+params['ModelParams']['dirTest'] = os.path.join(basePath, 'dataset/Test')
+# dataset from PROMISE12: prostate MRI scans, 30 testing
+params['ModelParams']['dirInfer'] = os.path.join(basePath, 'dataset/Infer')
+# where we need to save the results (relative to the base path)
+params['ModelParams']['dirResult'] = os.path.join(basePath, 'results')
+params['ModelParams']['dirSnapshots'] = os.path.join(
+    basePath, 'Models/MRI_cinque_snapshots/')  # where to save the models while training
 # params['ModelParams']['batchsize'] = 1  # the batchsize
 params['ModelParams']['numIterations'] = 100000  # the number of iterations
 params['ModelParams']['baseLR'] = 0.0001  # the learning rate, initial one
@@ -31,19 +36,19 @@ params['ModelParams']['nProc'] = 1  # the number of threads to do data augmentat
 
 
 #params of the DataManager
-params['DataManagerParams']['dstRes'] = np.asarray([1,1,1.5],dtype=float)
-params['DataManagerParams']['VolSize'] = np.asarray([64,64,32],dtype=int)
-params['DataManagerParams']['normDir'] = False  # if rotates the volume according to its transformation in the mhd file. Not reccommended.
+params['DataManagerParams']['dstRes'] = np.asarray([1, 1, 1.5], dtype=float)
+params['DataManagerParams']['VolSize'] = np.asarray([64, 64, 32], dtype=int)
+# if rotates the volume according to its transformation in the mhd file. Not reccommended.
+params['DataManagerParams']['normDir'] = False
 
 print('\n+preset parameters:\n' + str(params))
 
 
 #  parse sys.argv
 parser = argparse.ArgumentParser()
-parser.add_argument('--batchSz', type=int, default=1)
-parser.add_argument('--dice', action='store_true', default=True)
+parser.add_argument('--batchSz', type=int, default=16)
 parser.add_argument('--gpu_ids', type=list, default=[1])
-parser.add_argument('--nEpochs', type=int, default=50)
+parser.add_argument('--nEpochs', type=int, default=1)
 parser.add_argument('--start-epoch', default=0, type=int, metavar='N',
                     help='manual epoch number (useful on restarts)')
 parser.add_argument('--resume', default='', type=str, metavar='PATH',
@@ -56,6 +61,17 @@ parser.add_argument('--no-cuda', action='store_true', default=False)
 parser.add_argument('--seed', type=int, default=1)
 parser.add_argument('--opt', type=str, default='adam',
                     choices=('sgd', 'adam', 'rmsprop'))
+parser.add_argument('--ipex', action='store_true', default=False,
+                    help='use intel pytorch extension')
+parser.add_argument('--precision', type=str, default="float32",
+                    help='precision, float32, bfloat16')
+parser.add_argument('--jit', action='store_true', default=False,
+                    help='enable ipex jit fusionpath')
+parser.add_argument('--channels_last', type=int, default=1,
+                    help='use channels last format')
+parser.add_argument('--arch', type=str, default=None,
+                    help='model name')
+parser.add_argument('--profile', action='store_true', help='Trigger profile on current topology.')
 args = parser.parse_args()
 
 print('\n+sys arguments:\n' + str(args))
@@ -64,4 +80,3 @@ print('\n+sys arguments:\n' + str(args))
 
 #  load dataset, train, test(i.e. output predicted mask for test data in .mhd)
 train.main(params, args)
-
diff --git a/requirements.txt b/requirements.txt
index 35c9448..8299ac4 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,6 +1,4 @@
 numpy
-torch
-torchvision
 torchbiomed
 setproctitle
 graphviz
diff --git a/train.py b/train.py
index afde04e..d8a69e8 100644
--- a/train.py
+++ b/train.py
@@ -35,12 +35,35 @@ import make_graph
 from functools import reduce
 import operator
 
+
+def save_profile_result(filename, table):
+    import xlsxwriter
+    workbook = xlsxwriter.Workbook(filename)
+    worksheet = workbook.add_worksheet()
+    keys = ["Name", "Self CPU total %", "Self CPU total", "CPU total %" , "CPU total", \
+            "CPU time avg", "Number of Calls"]
+    for j in range(len(keys)):
+        worksheet.write(0, j, keys[j])
+
+    lines = table.split("\n")
+    for i in range(3, len(lines)-4):
+        words = lines[i].split(" ")
+        j = 0
+        for word in words:
+            if not word == "":
+                worksheet.write(i-2, j, word)
+                j += 1
+    workbook.close()
+
+
+
 def weights_init(m):
     classname = m.__class__.__name__
     if classname.find('Conv3d') != -1:
         nn.init.kaiming_normal_(m.weight)
         m.bias.data.zero_()
 
+
 def datestr():
     now = time.gmtime()
     return '{}{:02}{:02}_{:02}{:02}'.format(now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min)
@@ -55,6 +78,8 @@ def save_checkpoint(state, path, prefix, filename='checkpoint.pth.tar'):
 def inference(params, args, loader, model):
     src = params['ModelParams']['dirInfer']
     dst = params['ModelParams']['dirResult']
+    total_time = 0
+    i = 0
 
     model.eval()
     # assume single GPU / batch size 1
@@ -66,35 +91,95 @@ def inference(params, args, loader, model):
         spacing = np.array(list(reversed(itk_img.GetSpacing())))
 
         # pdb.set_trace()
-        _, _, z, y, x = data.shape # need to subset shape of 3-d. by Chao.
+        _, _, z, y, x = data.shape  # need to subset shape of 3-d. by Chao.
         # convert names to batch tensor
         if args.cuda:
             data.pin_memory()
             data = data.cuda()
-        with torch.no_grad():
-            data = Variable(data)
-        output = model(data)
-        _, output = output.max(1)
-        output = output.view((x, y, z))
-        # pdb.set_trace()
-        output = output.cpu()
-
-        print("save {}".format(id))
-        utils.save_updated_image(output, os.path.join(dst, id + "_predicted.mhd"), origin, spacing)
+        if args.ipex:
+            with torch.no_grad():
+                data = Variable(data)
+                data = data.to(device=ipex.DEVICE)
+                if args.jit:
+                    model = torch.jit.trace(model, data)
+                start = time.time()
+                output = model(data)
+                end = time.time()
+                total_time += end - start
+                _, output = output.max(1)
+                output = output.view((x, y, z))
+                # pdb.set_trace()
+                output = output.cpu()
+
+                print("save {}".format(id))
+                utils.save_updated_image(output, os.path.join(
+                    dst, id + "_predicted.mhd"), origin, spacing)
+                i += 1
+        else:
+            with torch.no_grad():
+                data = Variable(data)
+                if args.channels_last:
+                    data_oob = data
+                    try:
+                        data_oob = data_oob.to(memory_format=torch.channels_last)
+                    except:
+                        print("Input NHWC failed! Use normal input.")
+                    data = data_oob
+                    if args.jit:
+                        model = torch.jit.trace(model, data_oob)
+            start = time.time()
+            if args.profile:
+                with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU]) as prof:
+                    if args.precision == "bfloat16":
+                        with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                            output = model(data)
+                    else:
+                        output = model(data)
+            else:
+                if args.precision == "bfloat16":
+                    with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                        output = model(data)
+                else:
+                    output = model(data)
+            end = time.time()
+            total_time += end - start
+            _, output = output.max(1)
+            output = output.view((x, y, z))
+            # pdb.set_trace()
+            output = output.cpu()
+
+            print("save {}".format(id))
+            utils.save_updated_image(output, os.path.join(
+                dst, id + "_predicted.mhd"), origin, spacing)
+            i += 1
+
+    print('Throughput is: %f imgs/s' % (args.batchSz * i / total_time))
+    #
+    if args.profile:
+        import pathlib
+        timeline_dir = str(pathlib.Path.cwd()) + '/timeline/'
+        if not os.path.exists(timeline_dir):
+            os.makedirs(timeline_dir)
+        timeline_file = timeline_dir + 'timeline-' + str(torch.backends.quantized.engine) + '-' + \
+                    "vnet" + str(batch_idx) + '-' + str(os.getpid()) + '.json'
+        print(timeline_file)
+        prof.export_chrome_trace(timeline_file)
+        # table_res = prof.key_averages().table(sort_by="cpu_time_total")
+        # self.save_profile_result(timeline_dir + torch.backends.quantized.engine + "_result_average.xlsx", table_res)
 
 # performing post-train test:
 # train.py --resume <model checkpoint> --i <input directory (*.mhd)> --save <output directory>
 
+
 def noop(x):
     return x
 
+
 def main(params, args):
-    best_prec1 = 100. # accuracy? by Chao
+    best_prec1 = 100.  # accuracy? by Chao
     args.cuda = not args.no_cuda and torch.cuda.is_available()
     resultDir = 'results/vnet.base.{}'.format(datestr())
-    nll = True
-    if args.dice:
-        nll = False
+    nll = False
     weight_decay = args.weight_decay
     setproctitle.setproctitle(resultDir)
 
@@ -105,7 +190,7 @@ def main(params, args):
     print("build vnet")
     model = vnet.VNet(elu=False, nll=nll)
     batch_size = args.batchSz
-    torch.cuda.set_device(0) # why do I have to add this line? It seems the below line is useless to apply GPU devices. By Chao.
+    # torch.cuda.set_device(0) # why do I have to add this line? It seems the below line is useless to apply GPU devices. By Chao.
     model = nn.parallel.DataParallel(model, device_ids=[0])
 
     if args.resume:
@@ -122,20 +207,34 @@ def main(params, args):
     else:
         model.apply(weights_init)
 
-    if nll:
-        train = train_nll
-        test = test_nll
-    else:
-        train = train_dice
-        test = test_dice
+    train = train_dice
+    test = test_dice
 
     print('  + Number of params: {}'.format(
         sum([p.data.nelement() for p in model.parameters()])))
     if args.cuda:
         model = model.cuda()
-
-    if os.path.exists(resultDir):
-        shutil.rmtree(resultDir)
+    elif args.ipex:
+        import intel_pytorch_extension as ipex
+        model = model.to(device=ipex.DEVICE)
+        print("using ipex model to do inference\n")
+        if args.precision == "bfloat16":
+            ipex.enable_auto_mixed_precision(mixed_dtype=torch.bfloat16)
+            print("running bf16 evalation step\n")
+        else:
+            print("running fp32 evalation step\n")
+        if args.jit:
+            print("running jit fusion path\n")
+    elif args.channels_last:
+        model_oob = model
+        try:
+            model_oob = model_oob.to(memory_format=torch.channels_last)
+        except:
+            print("Model NHWC failed! Use normal model.")
+        model = model_oob
+
+    # if os.path.exists(resultDir):
+    #     shutil.rmtree(resultDir)
     os.makedirs(resultDir, exist_ok=True)
 
     # transform
@@ -150,24 +249,26 @@ def main(params, args):
 
     print("\nloading training set")
     dataManagerTrain = DM.DataManager(params['ModelParams']['dirTrain'],
-                                          params['ModelParams']['dirResult'],
-                                          params['DataManagerParams'])
-    dataManagerTrain.loadTrainingData() # required
+                                      params['ModelParams']['dirResult'],
+                                      params['DataManagerParams'])
+    dataManagerTrain.loadTrainingData()  # required
     numpyImages = dataManagerTrain.getNumpyImages()
     numpyGT = dataManagerTrain.getNumpyGT()
 
-    trainSet = promise12.PROMISE12(mode='train', images=numpyImages, GT=numpyGT, transform=trainTransform)
+    trainSet = promise12.PROMISE12(mode='train', images=numpyImages,
+                                   GT=numpyGT, transform=trainTransform)
     trainLoader = DataLoader(trainSet, batch_size=batch_size, shuffle=True, **kwargs)
 
     print("\nloading test set")
     dataManagerTest = DM.DataManager(params['ModelParams']['dirTest'],
-                                      params['ModelParams']['dirResult'],
-                                      params['DataManagerParams'])
+                                     params['ModelParams']['dirResult'],
+                                     params['DataManagerParams'])
     dataManagerTest.loadTestingData()  # required
     numpyImages = dataManagerTest.getNumpyImages()
     numpyGT = dataManagerTest.getNumpyGT()
 
-    testSet = promise12.PROMISE12(mode='test', images=numpyImages, GT=numpyGT, transform=testTransform)
+    testSet = promise12.PROMISE12(mode='test', images=numpyImages,
+                                  GT=numpyGT, transform=testTransform)
     testLoader = DataLoader(testSet, batch_size=batch_size, shuffle=True, **kwargs)
 
     if args.opt == 'sgd':
@@ -178,6 +279,7 @@ def main(params, args):
     elif args.opt == 'rmsprop':
         optimizer = optim.RMSprop(model.parameters(), weight_decay=weight_decay)
 
+    '''
     trainF = open(os.path.join(resultDir, 'train.csv'), 'w')
     testF = open(os.path.join(resultDir, 'test.csv'), 'w')
 
@@ -192,7 +294,7 @@ def main(params, args):
 
     trainF.close()
     testF.close()
-
+    '''
     # inference, i.e. output predicted mask for test data in .mhd
     if params['ModelParams']['dirInfer'] != '':
         print("loading inference data")
@@ -202,66 +304,11 @@ def main(params, args):
         dataManagerInfer.loadInferData()  # required.  Create .loadInferData??? by Chao.
         numpyImages = dataManagerInfer.getNumpyImages()
 
-        inferSet = promise12.PROMISE12(mode='infer', images=numpyImages, GT=None, transform=testTransform)
+        inferSet = promise12.PROMISE12(mode='infer', images=numpyImages,
+                                       GT=None, transform=testTransform)
         inferLoader = DataLoader(inferSet, batch_size=batch_size, shuffle=True, **kwargs)
         inference(params, args, inferLoader, model)
 
-# def train_nll(args, epoch, model, trainLoader, optimizer, trainF):
-#     model.train()
-#     nProcessed = 0
-#     nTrain = len(trainLoader.dataset)
-#     for batch_idx, output in enumerate(trainLoader):
-#         data, target, id = output
-#         if args.cuda:
-#             data, target = data.cuda(), target.cuda()
-#         data, target = Variable(data), Variable(target)
-#         optimizer.zero_grad()
-#         output = model(data)
-#         target = target.view(target.numel())
-#         loss = F.nll_loss(output, target)
-#         dice_loss = bioloss.dice_error(output, target)
-#         # make_graph.save('/tmp/t.dot', loss.creator); assert(False)
-#         loss.backward()
-#         optimizer.step()
-#         nProcessed += len(data)
-#         pred = output.data.max(1)[1]  # get the index of the max log-probability
-#         incorrect = pred.ne(target.data).cpu().sum()
-#         err = 100.*incorrect/target.numel()
-#         partialEpoch = epoch + batch_idx / len(trainLoader) - 1
-#         print('Train Epoch: {:.2f} [{}/{} ({:.0f}%)]\tLoss: {:.4f}\tError: {:.3f}\t Dice: {:.6f}'.format(
-#             partialEpoch, nProcessed, nTrain, 100. * batch_idx / len(trainLoader),
-#             loss.data[0], err, dice_loss))
-#
-#         trainF.write('{},{},{}\n'.format(partialEpoch, loss.data[0], err))
-#         trainF.flush()
-#
-# def test_nll(args, epoch, model, testLoader, optimizer, testF):
-#     model.eval()
-#     test_loss = 0
-#     dice_loss = 0
-#     incorrect = 0
-#     numel = 0
-#     for data, target in testLoader:
-#         if args.cuda:
-#             data, target = data.cuda(), target.cuda()
-#         data, target = Variable(data, volatile=True), Variable(target)
-#         target = target.view(target.numel())
-#         numel += target.numel()
-#         output = model(data)
-#         test_loss += F.nll_loss(output, target, weight=weights).data[0]
-#         dice_loss += bioloss.dice_error(output, target)
-#         pred = output.data.max(1)[1]  # get the index of the max log-probability
-#         incorrect += pred.ne(target.data).cpu().sum()
-#
-#     test_loss /= len(testLoader)  # loss function already averages over batch size
-#     dice_loss /= len(testLoader)
-#     err = 100.*incorrect/numel
-#     print('\nTest set: Average loss: {:.4f}, Error: {}/{} ({:.3f}%) Dice: {:.6f}\n'.format(
-#         test_loss, incorrect, numel, err, dice_loss))
-#
-#     testF.write('{},{},{}\n'.format(epoch, test_loss, err))
-#     testF.flush()
-#     return err
 
 def train_dice(args, epoch, model, trainLoader, optimizer, trainF):
     model.train()
@@ -270,7 +317,7 @@ def train_dice(args, epoch, model, trainLoader, optimizer, trainF):
     for batch_idx, output in enumerate(trainLoader):
         data, target, id = output
         # print("training with {}".format(id[0]))
-        target = target[0,:,:,:].view(-1) # right? added by Chao. 
+        target = target[0, :, :, :].view(-1)  # right? added by Chao.
         if args.cuda:
             data, target = data.cuda(), target.cuda()
 
@@ -280,24 +327,20 @@ def train_dice(args, epoch, model, trainLoader, optimizer, trainF):
         optimizer.zero_grad()
         output = model(data)
         # pdb.set_trace()
-        loss = bioloss.dice_loss(output, target)
+        # loss = bioloss.dice_loss(output, target)
         # make_graph.save('/tmp/t.dot', loss.creator); assert(False)
-        loss.backward()
+        # loss.backward()
         optimizer.step()
         nProcessed += len(data)
-        err = 100.*(1. - loss.data[0]) # loss.data[0] is dice coefficient? By Chao.
-        # partialEpoch = epoch + batch_idx / len(trainLoader) - 1
-        # print('Train Epoch: {:.2f} [{}/{} ({:.0f}%)]\tLoss: {:.8f}\tError: {:.8f}'.format(
-        #     partialEpoch, nProcessed, nTrain, 100. * batch_idx / len(trainLoader),
-        #     loss.data[0], err))
+        # err = 100.*(1. - loss.data[0]) # loss.data[0] is dice coefficient? By Chao.
 
-    print('\nFor trainning: Epoch: {} \tdice_coefficient: {:.4f}\tError: {:.4f}\n'.format(
-    epoch, loss.data[0], err))
+    print('\nFor trainning: Epoch: {}\n'.format(
+        epoch))
 
-        # trainF.write('{},{},{}\n'.format(partialEpoch, loss.data[0], err))
-    trainF.write('{},{},{}\n'.format(epoch, loss.data[0], err))
+    trainF.write('{}\n'.format(epoch))
     trainF.flush()
 
+
 def test_dice(args, epoch, model, testLoader, optimizer, testF):
     model.eval()
     test_dice = 0
@@ -305,7 +348,7 @@ def test_dice(args, epoch, model, testLoader, optimizer, testF):
     for batch_idx, output in enumerate(testLoader):
         data, target, id = output
         # print("testing with {}".format(id[0]))
-        target = target[0,:,:,:].view(-1) # right? added by Chao. 
+        target = target[0, :, :, :].view(-1)  # right? added by Chao.
         if args.cuda:
             data, target = data.cuda(), target.cuda()
         data = Variable(data)
@@ -317,17 +360,14 @@ def test_dice(args, epoch, model, testLoader, optimizer, testF):
 
     nTotal = len(testLoader)
     test_dice /= nTotal  # loss function already averages over batch size
-    err = 100.*incorrect/nTotal
-    # print('\nTest set: Average Dice Coeff: {:.4f}, Error: {}/{} ({:.0f}%)\n'.format(
-    #     test_loss, incorrect, nTotal, err))
-    #
-    # testF.write('{},{},{}\n'.format(epoch, test_loss, err))
+    err = 100. * incorrect / nTotal
     print('\nFor testing: Epoch:{}\tAverage Dice Coeff: {:.4f}\tError:{:.4f}\n'.format(epoch, test_dice, err))
 
     testF.write('{},{},{}\n'.format(epoch, test_dice, err))
     testF.flush()
     return test_dice
 
+
 def adjust_opt(optAlg, optimizer, epoch):
     if optAlg == 'sgd':
         if epoch < 150:
