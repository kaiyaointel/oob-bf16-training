diff --git a/examples/run_glue.py b/examples/run_glue.py
index 1f586b7c5..d557c8514 100644
--- a/examples/run_glue.py
+++ b/examples/run_glue.py
@@ -24,6 +24,7 @@ import os
 import random
 from dataclasses import dataclass, field
 from typing import Optional
+import time 
 
 import numpy as np
 import torch
@@ -43,6 +44,7 @@ from transformers import (
     get_linear_schedule_with_warmup,
 )
 from transformers import glue_compute_metrics as compute_metrics
+from transformers import bart_compute_metrics as bart_compute_metrics
 from transformers import glue_convert_examples_to_features as convert_examples_to_features
 from transformers import glue_output_modes as output_modes
 from transformers import glue_processors as processors
@@ -175,7 +177,7 @@ def train(args, train_dataset, model, tokenizer):
             model.train()
             batch = tuple(t.to(args.device) for t in batch)
             inputs = {"input_ids": batch[0], "attention_mask": batch[1], "labels": batch[3]}
-            if args.model_type != "distilbert":
+            if args.model_type not in ["distilbert", "roberta", "xlm-roberta", "xlm", "bart"]:
                 inputs["token_type_ids"] = (
                     batch[2] if args.model_type in ["bert", "xlnet", "albert"] else None
                 )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids
@@ -280,6 +282,30 @@ def evaluate(args, model, tokenizer, prefix=""):
         # multi-gpu eval
         if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):
             model = torch.nn.DataParallel(model)
+        if args.ipex:
+            # Import Extension
+            import intel_pytorch_extension as ipex
+            print("Running with IPEX...")
+            if args.precision == 'bfloat16':
+                # Automatically mix precision
+                ipex.enable_auto_mixed_precision(mixed_dtype=torch.bfloat16)
+                print("Running with bfloat16...")
+            model = model.to(ipex.DEVICE)
+
+        if args.precision == "bfloat16":
+            # with torch.amp.autocast(enabled=True, configure=torch.bfloat16, torch.no_grad(): 
+            print("Running with bfloat16...")
+
+        ### to_oob
+        model_oob = model
+        if args.channels_last:
+            model_oob = model_oob.to(memory_format=torch.channels_last)
+        if args.jit:
+            try:
+                model_oob = torch.jit.trace(model_oob.eval(),input_oob)
+            except:
+                print("Can't convert to jit model...")
+        model = model_oob
 
         # Eval!
         logger.info("***** Running evaluation {} *****".format(prefix))
@@ -289,17 +315,91 @@ def evaluate(args, model, tokenizer, prefix=""):
         nb_eval_steps = 0
         preds = None
         out_label_ids = None
+        # for warmup
+        for num_iter, batch in enumerate(eval_dataloader):
+            print("*", end="")
+            model.eval()
+            with torch.no_grad():
+                inputs = {"input_ids": batch[0], "attention_mask": batch[1], "labels": batch[3]}
+                if args.model_type not in ["distilbert", "roberta", "xlm-roberta", "xlm", "bart"]:
+                    inputs["token_type_ids"] = (
+                        batch[2] if args.model_type in ["bert", "xlnet", "albert"] else None
+                    )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids
+                if args.ipex:
+                    inputs = {key:value.to(ipex.DEVICE) for key,value in inputs.items()}
+                if args.ipex and args.jit:
+                    if args.model_type in ["distilbert", "roberta", "xlm-roberta", "xlm", "bart"]:
+                        model = torch.jit.trace(model, (inputs['input_ids'], inputs['attention_mask'], inputs['labels']))
+                    else:
+                        model = torch.jit.trace(model, (inputs['input_ids'], inputs['attention_mask'], inputs['token_type_ids'], inputs['labels']))
+
+                ### to_oob
+                input_oob = inputs
+                if args.channels_last:
+                    input_oob = {key:value.to(memory_format=torch.channels_last) for key,value in inputs.items()}
+
+                if args.jit:
+                    try:
+                        model_oob = torch.jit.trace(model_oob.eval(),input_oob)
+                    except:
+                        print("Can't convert to jit model...")
+                inputs = input_oob
+
+                outputs = model(**inputs)
+                tmp_eval_loss, logits = outputs[:2]
+                eval_loss += tmp_eval_loss.mean().item()
+
+            if num_iter > args.num_warmup_iters:
+                print("Complete %d iters warmup" % args.num_warmup_iters)
+                break
+
+        total_batch = 0
+        total_time = 0
         for batch in tqdm(eval_dataloader, desc="Evaluating"):
             model.eval()
             batch = tuple(t.to(args.device) for t in batch)
 
             with torch.no_grad():
                 inputs = {"input_ids": batch[0], "attention_mask": batch[1], "labels": batch[3]}
-                if args.model_type != "distilbert":
+                if args.model_type not in ["distilbert", "roberta", "xlm-roberta", "xlm", "bart"]:
                     inputs["token_type_ids"] = (
                         batch[2] if args.model_type in ["bert", "xlnet", "albert"] else None
                     )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids
-                outputs = model(**inputs)
+                if args.ipex:
+                    inputs = {key:value.to(ipex.DEVICE) for key,value in inputs.items()}
+                if args.ipex and args.jit:
+                    if args.model_type in ["distilbert", "roberta", "xlm-roberta", "xlm", "bart"]:
+                        model = torch.jit.trace(model, (inputs['input_ids'], inputs['attention_mask'], inputs['labels']))
+                    else:
+                        model = torch.jit.trace(model, (inputs['input_ids'], inputs['attention_mask'], inputs['token_type_ids'], inputs['labels']))
+
+                ### to_oob
+                model_oob, input_oob = model, inputs
+                if args.channels_last:
+                    input_oob = {key:value.to(memory_format=torch.channels_last) for key,value in inputs.items()}
+
+                if args.jit:
+                    try:
+                        model_oob = torch.jit.trace(model_oob.eval(), input_oob)
+                    except:
+                        print("Can't convert to jit model...")
+                model, inputs = model_oob, input_oob
+
+                tic = time.time()
+                if args.profile:
+                    with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU]) as prof:
+                        if args.precision == "bfloat16":
+                            with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                                outputs = model(**inputs)
+                        else:
+                            outputs = model(**inputs)
+                else:
+                    if args.precision == "bfloat16":
+                        with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                            outputs = model(**inputs)
+                    else:
+                        outputs = model(**inputs)
+                total_time += time.time() - tic
                 tmp_eval_loss, logits = outputs[:2]
 
                 eval_loss += tmp_eval_loss.mean().item()
@@ -310,13 +410,31 @@ def evaluate(args, model, tokenizer, prefix=""):
             else:
                 preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)
                 out_label_ids = np.append(out_label_ids, inputs["labels"].detach().cpu().numpy(), axis=0)
+            total_batch += 1
+            if total_batch >= args.num_iters:
+                break
+        #
+        if args.profile:
+            import pathlib
+            timeline_dir = str(pathlib.Path.cwd()) + '/timeline/'
+            if not os.path.exists(timeline_dir):
+                os.makedirs(timeline_dir)
+            timeline_file = timeline_dir + 'timeline-' + str(torch.backends.quantized.engine) + '-' + \
+                        args.model_type + '-' + str(os.getpid()) + '.json'
+            print(timeline_file)
+            prof.export_chrome_trace(timeline_file)
+            # table_res = prof.key_averages().table(sort_by="cpu_time_total")
+            # save_profile_result(timeline_dir + torch.backends.quantized.engine + "_result_average.xlsx", table_res)
 
         eval_loss = eval_loss / nb_eval_steps
         if args.output_mode == "classification":
             preds = np.argmax(preds, axis=1)
         elif args.output_mode == "regression":
             preds = np.squeeze(preds)
-        result = compute_metrics(eval_task, preds, out_label_ids)
+        if args.model_type in ["bart"]:
+            result = bart_compute_metrics(eval_task, preds, out_label_ids)
+        else:
+            result = compute_metrics(eval_task, preds, out_label_ids)
         results.update(result)
 
         output_eval_file = os.path.join(eval_output_dir, prefix, "eval_results.txt")
@@ -326,6 +444,10 @@ def evaluate(args, model, tokenizer, prefix=""):
                 logger.info("  %s = %s", key, str(result[key]))
                 writer.write("%s = %s\n" % (key, str(result[key])))
 
+        data_sum = args.eval_batch_size * total_batch
+        print(" time cost %s\n inference latency: %ss\n inference Throughput: %s samples/s\n " 
+              %(total_time, total_time / data_sum, data_sum / total_time))
+
     return results
 
 
@@ -370,7 +492,10 @@ def load_and_cache_examples(args, task, tokenizer, evaluate=False):
     # Convert to Tensors and build dataset
     all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)
     all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)
-    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)
+    if args.model_type in ["distilbert", "roberta", "xlm-roberta", "xlm", "bart"]:
+        all_token_type_ids = torch.tensor([0 for f in features], dtype=torch.long)
+    else:
+        all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)
     if output_mode == "classification":
         all_labels = torch.tensor([f.label for f in features], dtype=torch.long)
     elif output_mode == "regression":
@@ -428,6 +553,7 @@ def main():
     # For now, let's merge all the sets of args into one,
     # but soon, we'll keep distinct sets of args, with a cleaner separation of concerns.
     args = argparse.Namespace(**vars(model_args), **vars(dataprocessing_args), **vars(training_args))
+    print(args)
 
     if (
         os.path.exists(args.output_dir)
@@ -556,6 +682,24 @@ def main():
             results.update(result)
 
     return results
+def save_profile_result(filename, table):
+    import xlsxwriter
+    workbook = xlsxwriter.Workbook(filename)
+    worksheet = workbook.add_worksheet()
+    keys = ["Name", "Self CPU total %", "Self CPU total", "CPU total %" , "CPU total", \
+            "CPU time avg", "Number of Calls"]
+    for j in range(len(keys)):
+        worksheet.write(0, j, keys[j])
+
+    lines = table.split("\n")
+    for i in range(3, len(lines)-4):
+        words = lines[i].split(" ")
+        j = 0
+        for word in words:
+            if not word == "":
+                worksheet.write(i-2, j, word)
+                j += 1
+    workbook.close()
 
 
 if __name__ == "__main__":
diff --git a/src/transformers/__init__.py b/src/transformers/__init__.py
index 01f560016..7bc7813e4 100755
--- a/src/transformers/__init__.py
+++ b/src/transformers/__init__.py
@@ -149,7 +149,7 @@ logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
 
 if is_sklearn_available():
-    from .data import glue_compute_metrics, xnli_compute_metrics
+    from .data import glue_compute_metrics, xnli_compute_metrics, bart_compute_metrics
 
 
 # Modeling
diff --git a/src/transformers/data/__init__.py b/src/transformers/data/__init__.py
index 8d5f6b85b..3f2e88d25 100644
--- a/src/transformers/data/__init__.py
+++ b/src/transformers/data/__init__.py
@@ -24,4 +24,4 @@ from .processors import (
 
 
 if is_sklearn_available():
-    from .metrics import glue_compute_metrics, xnli_compute_metrics
+    from .metrics import glue_compute_metrics, xnli_compute_metrics, bart_compute_metrics
diff --git a/src/transformers/data/metrics/__init__.py b/src/transformers/data/metrics/__init__.py
index 6c29c2313..575130403 100644
--- a/src/transformers/data/metrics/__init__.py
+++ b/src/transformers/data/metrics/__init__.py
@@ -83,3 +83,10 @@ if _has_sklearn:
             return {"acc": simple_accuracy(preds, labels)}
         else:
             raise KeyError(task_name)
+
+    def bart_compute_metrics(task_name, preds, labels):
+        assert len(preds) == len(labels)
+        if task_name == "mrpc":
+            return {"acc": simple_accuracy(preds, labels)}
+        else:
+            raise KeyError(task_name)
diff --git a/src/transformers/training_args.py b/src/transformers/training_args.py
index b48486dfb..deb2ed3ed 100644
--- a/src/transformers/training_args.py
+++ b/src/transformers/training_args.py
@@ -73,3 +73,15 @@ class TrainingArguments:
         },
     )
     local_rank: int = field(default=-1, metadata={"help": "For distributed training: local_rank"})
+    num_warmup_iters: int = field(default=10, metadata={"help": "Warmup steps for evaluation benchmarking."})
+    num_iters: int = field(default=500, metadata={"help": "total iters for evaluation benchmarking."})
+    profiling: bool = field(default=False, metadata={"help": "Doing profiling on cpu."})
+    ipex: bool = field(default=False, metadata={"help": "Use Intel IPEX."})
+    jit: bool = field(default=False, metadata={"help": "Use jit optimize to do optimization."})
+    precision: str = field(default='float32',
+        metadata={"help": "Precision: 'float32' or 'bfloat16'."})
+    arch: str = field(default='',
+        metadata={"help": "model name."})
+    channels_last: int = field(default=1, metadata={"help": "set 1 to use NHWC data format."})
+    profile: bool = field(default=False, metadata={"help": "Use Intel profile."})
+
