diff --git a/single_stage_detector/ssd/ssd300.py b/single_stage_detector/ssd/ssd300.py
index a4cd3e5..aec70b9 100644
--- a/single_stage_detector/ssd/ssd300.py
+++ b/single_stage_detector/ssd/ssd300.py
@@ -123,7 +123,7 @@ class SSD300(nn.Module):
     def bbox_view(self, src, loc, conf):
         ret = []
         for s, l, c in zip(src, loc, conf):
-            ret.append((l(s).view(s.size(0), 4, -1), c(s).view(s.size(0), self.label_num, -1)))
+            ret.append((l(s).reshape(s.size(0), 4, -1), c(s).reshape(s.size(0), self.label_num, -1)))
 
         locs, confs = list(zip(*ret))
         locs, confs = torch.cat(locs, 2).contiguous(), torch.cat(confs, 2).contiguous()
diff --git a/single_stage_detector/ssd/train.py b/single_stage_detector/ssd/train.py
index badfe52..b1ec931 100644
--- a/single_stage_detector/ssd/train.py
+++ b/single_stage_detector/ssd/train.py
@@ -12,6 +12,11 @@ import random
 import numpy as np
 from mlperf_compliance import mlperf_log
 from mlperf_logger import ssd_print, broadcast_seeds
+try:
+    import intel_pytorch_extension as ipex
+    USE_IPEX = True
+except:
+    USE_IPEX = False
 
 def parse_args():
     parser = ArgumentParser(description="Train Single Shot MultiBox Detector"
@@ -34,9 +39,21 @@ def parse_args():
                         help='path to model checkpoint file')
     parser.add_argument('--no-save', action='store_true',
                         help='save model checkpoints')
+    parser.add_argument('--ipex', action='store_true', default=False,
+                        help='enable Intel_PyTorch_Extension')
+    parser.add_argument('--precision', type=str, default='float32',
+                        help='data type precision, default is float32.')
+    parser.add_argument('--channels_last', type=int, default=1,
+                        help='use channels last format')
+    parser.add_argument('--arch', type=str, default="ssd300",
+                        help='model name')
     parser.add_argument('--evaluation', nargs='*', type=int,
-                        default=[40, 50, 55, 60, 65, 70, 75, 80],
+                        default=[10, 20, 40, 50, 55, 60, 65, 70, 75, 80],
                         help='epochs at which to evaluate')
+    parser.add_argument('--eval-only', action='store_true',
+                        help='do evaluation only')
+    parser.add_argument('--bench-mark', action='store_true', default=True,
+                        help='bench-mark only')
     parser.add_argument('--lr-decay-schedule', nargs='*', type=int,
                         default=[40, 50],
                         help='epochs at which to decay the learning rate')
@@ -44,16 +61,77 @@ def parse_args():
                         help='how long the learning rate will be warmed up in fraction of epochs')
     parser.add_argument('--warmup-factor', type=int, default=0,
                         help='mlperf rule parameter for controlling warmup curve')
+    parser.add_argument('--perf-prerun-warmup', type=int, default=5,
+                        help='how much iterations to pre run before performance test, -1 mean use all dataset.')
+    parser.add_argument('--perf-run-iters', type=int, default=0,
+                        help='how much iterations to run performance test, 0 mean use all dataset.')
     parser.add_argument('--lr', type=float, default=2.5e-3,
                         help='base learning rate')
     # Distributed stuff
     parser.add_argument('--local_rank', default=0, type=int,
                         help='Used for multi-process training. Can either be manually set ' +
                         'or automatically set by using \'python -m multiproc\'.')
-
+    parser.add_argument('--profile', action='store_true', default=False)
     return parser.parse_args()
 
 
+def save_time(file_name, content):
+    import json
+    #file_name = os.path.join(args.log, 'result_' + str(idx) + '.json')
+    finally_result = []
+    with open(file_name, "r",encoding="utf-8") as f:
+        data = json.loads(f.read())
+        finally_result += data
+        finally_result += content
+        with open(file_name,"w",encoding="utf-8") as f:
+            f.write(json.dumps(finally_result,ensure_ascii=False,indent=2))
+
+
+def save_profile_result(filename, table):
+    import xlsxwriter
+    workbook = xlsxwriter.Workbook(filename)
+    worksheet = workbook.add_worksheet()
+    keys = ["Name", "Self CPU total %", "Self CPU total", "CPU total %" , "CPU total", \
+            "CPU time avg", "Number of Calls"]
+    for j in range(len(keys)):
+        worksheet.write(0, j, keys[j])
+
+    lines = table.split("\n")
+    for i in range(3, len(lines)-4):
+        words = lines[i].split(" ")
+        j = 0
+        for word in words:
+            if not word == "":
+                time.sleep(0.1)
+                worksheet.write(i-2, j, word)
+                j += 1
+    workbook.close()
+
+
+class AverageMeter(object):
+    """Computes and stores the average and current value"""
+    def __init__(self, name, fmt=':f'):
+        self.name = name
+        self.fmt = fmt
+        self.reset()
+
+    def reset(self):
+        self.val = 0
+        self.avg = 0
+        self.sum = 0
+        self.count = 0
+
+    def update(self, val, n=1):
+        self.val = val
+        self.sum += val * n
+        self.count += n
+        self.avg = self.sum / self.count
+
+    def __str__(self):
+        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'
+        return fmtstr.format(**self.__dict__)
+
+
 def show_memusage(device=0):
     import gpustat
     gpu_stats = gpustat.GPUStatCollection.new_query()
@@ -83,12 +161,22 @@ def dboxes300_coco():
 
 
 def coco_eval(model, coco, cocoGt, encoder, inv_map, threshold,
-              epoch, iteration, use_cuda=True):
+              epoch, iteration, args, use_cuda=True, use_ipex=False, bench_mark=True):
     from pycocotools.cocoeval import COCOeval
+
+    batch_time = AverageMeter('Time', ':6.3f')
+
     print("")
     model.eval()
     if use_cuda:
         model.cuda()
+    elif args.channels_last:
+        model_oob = model
+        model_oob = model_oob.to(memory_format=torch.channels_last)
+        model = model_oob
+        print("---- Use channels last format.")
+    elif use_ipex:
+        model.to(ipex.DEVICE)
     ret = []
 
     overlap_threshold = 0.50
@@ -100,8 +188,24 @@ def coco_eval(model, coco, cocoGt, encoder, inv_map, threshold,
 
     ssd_print(key=mlperf_log.EVAL_START, value=epoch, sync=False)
 
+    if use_ipex:
+        for idx, image_id in enumerate(coco.img_keys):
+            img, (htot, wtot), _, _ = coco[idx]
+            inp = img.unsqueeze(0).to(ipex.DEVICE)
+            model = torch.jit.trace(model, inp)
+            break
+    elif args.channels_last:
+        for idx, image_id in enumerate(coco.img_keys):
+            img, (htot, wtot), _, _ = coco[idx]
+            inp_oob = img.unsqueeze(0).to(memory_format=torch.channels_last)
+            model_oob = torch.jit.trace(model_oob, inp_oob)
+            break
+        model = model_oob
+
     start = time.time()
     for idx, image_id in enumerate(coco.img_keys):
+        if args.perf_run_iters != 0 and idx >= args.perf_run_iters:
+            break
         img, (htot, wtot), _, _ = coco[idx]
 
         with torch.no_grad():
@@ -109,18 +213,40 @@ def coco_eval(model, coco, cocoGt, encoder, inv_map, threshold,
             inp = img.unsqueeze(0)
             if use_cuda:
                 inp = inp.cuda()
+            elif args.channels_last:
+                inp_oob = inp
+                inp_oob = inp_oob.to(memory_format=torch.channels_last)
+                inp = inp_oob
+            elif use_ipex:
+                inp = inp.to(ipex.DEVICE)
+
+            if args.perf_prerun_warmup > 0 and idx >= args.perf_prerun_warmup:
+                        start_time=time.time()
             ploc, plabel = model(inp)
 
             try:
-                result = encoder.decode_batch(ploc, plabel,
-                                              overlap_threshold,
-                                              nms_max_detections)[0]
+                if args.profile:
+                    with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU]) as prof:
+                        if args.precision == "bfloat16":
+                            with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                                result = encoder.decode_batch(ploc, plabel, overlap_threshold, nms_max_detections)[0]
+                        else:
+                            result = encoder.decode_batch(ploc, plabel, overlap_threshold, nms_max_detections)[0]
+                else:
+                    if args.precision == "bfloat16":
+                        with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                            result = encoder.decode_batch(ploc, plabel, overlap_threshold, nms_max_detections)[0]
+                    else:
+                        result = encoder.decode_batch(ploc, plabel, overlap_threshold, nms_max_detections)[0]
 
             except:
                 #raise
                 print("")
                 print("No object detected in idx: {}".format(idx))
                 continue
+            finally:
+                if idx >= args.perf_prerun_warmup:
+                    batch_time.update(time.time()-start_time)
 
             loc, label, prob = [r.cpu().numpy() for r in result]
             for loc_, label_, prob_ in zip(loc, label, prob):
@@ -131,8 +257,23 @@ def coco_eval(model, coco, cocoGt, encoder, inv_map, threshold,
                                       prob_,
                                       inv_map[label_]])
     print("")
-    print("Predicting Ended, total time: {:.2f} s".format(time.time()-start))
-
+    latency = batch_time.avg / 1 * 1000
+    perf = 1 / batch_time.avg
+    print('inference latency %3.0f ms'%latency)
+    print('inference Throughput: %3.0f fps'%perf)
+    if args.profile:
+        import pathlib
+        timeline_dir = str(pathlib.Path.cwd()) + '/timeline/'
+        if not os.path.exists(timeline_dir):
+            os.makedirs(timeline_dir)
+        timeline_file = timeline_dir + 'timeline-' + str(torch.backends.quantized.engine) + '-' + \
+                    args.arch + str(idx) + '-' + str(os.getpid()) + '.json'
+        print(timeline_file)
+        prof.export_chrome_trace(timeline_file)
+        # table_res = prof.key_averages().table(sort_by="cpu_time_total")
+        # self.save_profile_result(timeline_dir + torch.backends.quantized.engine + "_result_average.xlsx", table_res)
+    if bench_mark:
+       return
     cocoDt = cocoGt.loadRes(np.array(ret))
 
     E = COCOeval(cocoGt, cocoDt, iouType='bbox')
@@ -172,6 +313,14 @@ def train300_mlperf_coco(args):
     from coco import COCO
     # Check that GPUs are actually available
     use_cuda = not args.no_cuda and torch.cuda.is_available()
+
+    if args.ipex:
+        assert USE_IPEX, "No module: intel_pytorch_extension"
+        if args.precision == "bfloat16":
+            # Automatically mix precision
+            ipex.enable_auto_mixed_precision(mixed_dtype = torch.bfloat16)
+            print("Running with bfloat16...")
+
     args.distributed = False
     if use_cuda:
         try:
@@ -187,7 +336,13 @@ def train300_mlperf_coco(args):
         import torch.distributed as dist
  #     ssd_print(key=mlperf_log.RUN_SET_RANDOM_SEED)
         if args.no_cuda:
-            device = torch.device('cpu')
+            if args.ipex:
+               print("run on ipex path...")
+               ipex.enable_auto_optimization()
+               device = torch.device('dpcpp')
+            else:
+               device = torch.device('cpu')
+
         else:
             torch.cuda.set_device(args.local_rank)
             device = torch.device('cuda')
@@ -235,6 +390,10 @@ def train300_mlperf_coco(args):
     ssd_print(key=mlperf_log.INPUT_BATCH_SIZE, value=args.batch_size)
 
 
+    if args.precision == "bfloat16":
+        # with torch.amp.autocast(enabled=True, configure=torch.bfloat16, torch.no_grad(): 
+        print("Running with bfloat16...")
+
     ssd300 = SSD300(train_coco.labelnum)
     if args.checkpoint is not None:
         print("loading model checkpoint", args.checkpoint)
@@ -243,9 +402,17 @@ def train300_mlperf_coco(args):
     ssd300.train()
     if use_cuda:
         ssd300.cuda()
+    if args.channels_last:
+        ssd300_oob = ssd300
+        ssd300_oob = ssd300_oob.to(memory_format=torch.channels_last)
+        ssd300 = ssd300_oob
+    if USE_IPEX:
+        ssd300.to(ipex.DEVICE)
     loss_func = Loss(dboxes)
     if use_cuda:
         loss_func.cuda()
+    if USE_IPEX:
+        loss_func.to(ipex.DEVICE)
     if args.distributed:
         N_gpu = torch.distributed.get_world_size()
     else:
@@ -276,7 +443,8 @@ def train300_mlperf_coco(args):
     success = torch.zeros(1)
     if use_cuda:
         success = success.cuda()
-
+    elif USE_IPEX:
+        success = success.to(ipex.DEVICE)
 
     if args.warmup:
         nonempty_imgs = len(train_coco)
@@ -299,32 +467,47 @@ def train300_mlperf_coco(args):
                 param_group['lr'] = current_lr
             ssd_print(key=mlperf_log.OPT_LR,
                                  value=current_lr)
-
-        for nbatch, (img, img_size, bbox, label) in enumerate(train_dataloader):
-
-            if use_cuda:
-                img = img.cuda()
-            img = Variable(img, requires_grad=True)
-            ploc, plabel = ssd300(img)
-            trans_bbox = bbox.transpose(1,2).contiguous()
-            if use_cuda:
-                trans_bbox = trans_bbox.cuda()
-                label = label.cuda()
-            gloc, glabel = Variable(trans_bbox, requires_grad=False), \
-                           Variable(label, requires_grad=False)
-            loss = loss_func(ploc, plabel, gloc, glabel)
-
-            if not np.isinf(loss.item()): avg_loss = 0.999*avg_loss + 0.001*loss.item()
-
-            print("Iteration: {:6d}, Loss function: {:5.3f}, Average Loss: {:.3f}"\
-                        .format(iter_num, loss.item(), avg_loss), end="\r")
-            optim.zero_grad()
-            loss.backward()
-            warmup_step(iter_num, current_lr)
-            optim.step()
-
-            iter_num += 1
-
+        if not args.eval_only:
+            for nbatch, (img, img_size, bbox, label) in enumerate(train_dataloader):
+                if use_cuda:
+                    img = img.cuda()
+                elif args.channels_last:
+                    img_oob = img
+                    img_oob = img_oob.to(memory_format=torch.channels_last)
+                    img = img_oob
+                elif USE_IPEX:
+                    img = img.to(ipex.DEVICE)
+                img = Variable(img, requires_grad=True)
+                ploc, plabel = ssd300(img)
+                trans_bbox = bbox.transpose(1,2).contiguous()
+                if use_cuda:
+                    trans_bbox = trans_bbox.cuda()
+                    label = label.cuda()
+                elif args.channels_last:
+                    trans_bbox_oob = trans_bbox
+                    trans_bbox_oob = trans_bbox_oob.to(memory_format=torch.channels_last)
+                    trans_bbox = trans_bbox_oob
+                    label_oob = label
+                    label_oob = label_oob.to(memory_format=torch.channels_last)
+                    label = label_oob
+                elif USE_IPEX:
+                    trans_bbox = trans_bbox.to(ipex.DEVICE)
+                    label = label.to(ipex.DEVICE)
+
+                gloc, glabel = Variable(trans_bbox, requires_grad=False), \
+                               Variable(label, requires_grad=False)
+                loss = loss_func(ploc, plabel, gloc, glabel)
+
+                if not np.isinf(loss.item()): avg_loss = 0.999*avg_loss + 0.001*loss.item()
+
+                print("Iteration: {:6d}, Loss function: {:5.3f}, Average Loss: {:.3f}"\
+                            .format(iter_num, loss.item(), avg_loss), end="\r")
+                optim.zero_grad()
+                loss.backward()
+                warmup_step(iter_num, current_lr)
+                optim.step()
+
+                iter_num += 1
         if epoch + 1 in eval_points:
             rank = dist.get_rank() if args.distributed else args.local_rank
             if args.distributed:
@@ -339,12 +522,13 @@ def train300_mlperf_coco(args):
                     print("saving model...")
                     torch.save({"model" : ssd300.state_dict(), "label_map": train_coco.label_info},
                                "./models/iter_{}.pt".format(iter_num))
-
                 if coco_eval(ssd300, val_coco, cocoGt, encoder, inv_map,
-                            args.threshold, epoch + 1,iter_num):
+                            args.threshold, epoch + 1,iter_num, args, use_cuda, USE_IPEX, args.bench_mark):
                     success = torch.ones(1)
                     if use_cuda:
                         success = success.cuda()
+                    elif USE_IPEX:
+                        success = success.to(ipex.DEVICE)
             if args.distributed:
                 dist.broadcast(success, 0)
             if success[0]:
@@ -355,7 +539,7 @@ def train300_mlperf_coco(args):
 def main():
     args = parse_args()
 
-    if args.local_rank == 0:
+    if args.local_rank == 0 and not args.no_save:
         if not os.path.isdir('./models'):
             os.mkdir('./models')
 
@@ -363,7 +547,6 @@ def main():
 
     # start timing here
     ssd_print(key=mlperf_log.RUN_START)
-
     success = train300_mlperf_coco(args)
 
     # end timing here
