diff --git a/dcgan/main.py b/dcgan/main.py
index b426db3..74ac6bb 100644
--- a/dcgan/main.py
+++ b/dcgan/main.py
@@ -11,7 +11,9 @@ import torch.utils.data
 import torchvision.datasets as dset
 import torchvision.transforms as transforms
 import torchvision.utils as vutils
-
+from torch.autograd import Variable
+import numpy as np
+import time
 
 parser = argparse.ArgumentParser()
 parser.add_argument('--dataset', required=True, help='cifar10 | lsun | mnist |imagenet | folder | lfw | fake')
@@ -32,6 +34,16 @@ parser.add_argument('--netD', default='', help="path to netD (to continue traini
 parser.add_argument('--outf', default='.', help='folder to output images and model checkpoints')
 parser.add_argument('--manualSeed', type=int, help='manual seed')
 parser.add_argument('--classes', default='bedroom', help='comma separated list of classes for the lsun data set')
+parser.add_argument("--arch", type=str, default="", help="model name")
+parser.add_argument('--inference', action='store_true', default=False)
+parser.add_argument('--num-warmup', default=10, type=int)
+parser.add_argument('--num-iterations', default=100, type=int)
+parser.add_argument('--ipex', action='store_true', default=False)
+parser.add_argument('--precision', default='float32', help='Precision, "float32" or "bfloat16"')
+parser.add_argument('--jit', action='store_true', default=False)
+parser.add_argument('--profile', action='store_true', default=False ,help='Trigger profile on current topology.')
+parser.add_argument('--channels_last', type=int, default=1, help='use channels last format')
+parser.add_argument('--config_file', type=str, default='./conf.yaml', help='config file for int8 tuning')
 
 opt = parser.parse_args()
 print(opt)
@@ -118,6 +130,12 @@ def weights_init(m):
         m.weight.data.normal_(1.0, 0.02)
         m.bias.data.fill_(0)
 
+class _DataLoader(object):
+    def __init__(self, data=None, batch_size=1):
+        self.data = data
+        self.batch_size = batch_size
+    def __iter__(self):
+        yield self.data[0], self.data[1]
 
 class Generator(nn.Module):
     def __init__(self, ngpu):
@@ -147,10 +165,10 @@ class Generator(nn.Module):
         )
 
     def forward(self, input):
-        if input.is_cuda and self.ngpu > 1:
-            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))
-        else:
-            output = self.main(input)
+        # if input.is_cuda and self.ngpu > 1:
+        #     output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))
+        # else:
+        output = self.main(input)
         return output
 
 
@@ -158,7 +176,7 @@ netG = Generator(ngpu).to(device)
 netG.apply(weights_init)
 if opt.netG != '':
     netG.load_state_dict(torch.load(opt.netG))
-print(netG)
+# print(netG)
 
 
 class Discriminator(nn.Module):
@@ -194,6 +212,77 @@ class Discriminator(nn.Module):
 
         return output.view(-1, 1).squeeze(1)
 
+FloatTensor = torch.cuda.FloatTensor if opt.cuda else torch.FloatTensor
+LongTensor = torch.cuda.LongTensor if opt.cuda else torch.LongTensor
+
+
+def generate(netG, batchsize, device):
+    fixed_noise = torch.randn(opt.batchSize, nz, 1, 1, device=device)
+    if opt.channels_last:
+        netG_oob, fixed_noise_oob = netG, fixed_noise
+        netG_oob = netG_oob.to(memory_format=torch.channels_last)
+        try:
+            fixed_noise_oob = fixed_noise_oob.to(memory_format=torch.channels_last)
+        except:
+            print("Input NHWC failed! Use normal input.")
+        netG, fixed_noise = netG_oob, fixed_noise_oob
+    else:
+        netG = netG.to(device=device)
+        fixed_noise = fixed_noise.to(device=device)
+    labels = np.array([num for _ in range(10) for num in range(10)])
+    labels = Variable(LongTensor(labels))
+
+    if opt.precision == 'int8':
+        from lpot.experimental import Quantization, common
+        quantizer = Quantization((opt.config_file))
+        dataset = (fixed_noise, labels)
+        calib_dataloader = _DataLoader(dataset)
+        quantizer.calib_dataloader = calib_dataloader
+        quantizer.model = common.Model(netG)
+        q_model = quantizer()
+        netG = q_model.model
+
+    netG.eval()
+
+    if opt.jit:
+        netG = torch.jit.trace(netG, (fixed_noise, labels))
+    with torch.no_grad():
+        for i in range(opt.num_warmup + opt.num_iterations):
+            if i == opt.num_warmup:
+                tic = time.time()
+            if opt.profile:
+                with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU]) as prof:
+                    if opt.precision == "bfloat16":
+                        with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                            fake = netG(fixed_noise)
+                    else:
+                        fake = netG(fixed_noise)
+            else:
+                if opt.precision == "bfloat16":
+                    with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                        fake = netG(fixed_noise)
+                else:
+                    fake = netG(fixed_noise)
+    toc = time.time() - tic
+    print("Throughput: %.2f image/sec, batchsize: %d, latency = %.2f ms"%((opt.num_iterations*batchsize)/toc, batchsize, 1000*toc/opt.num_iterations))
+    #
+    if opt.profile:
+        import pathlib
+        timeline_dir = str(pathlib.Path.cwd()) + '/timeline/'
+        if not os.path.exists(timeline_dir):
+            os.makedirs(timeline_dir)
+        timeline_file = timeline_dir + 'timeline-' + str(torch.backends.quantized.engine) + '-' + \
+                    opt.arch + '-' + str(i + 1) + '-' + str(os.getpid()) + '.json'
+        prof.export_chrome_trace(timeline_file)
+        # table_res = prof.key_averages().table(sort_by="cpu_time_total")
+        # save_profile_result(timeline_dir + torch.backends.quantized.engine + "_result_average.xlsx", table_res)
+
+if opt.inference:
+    print("----------------Generation benchmarking---------------")
+    generate(netG, opt.batchSize, device=torch.device('cpu'))
+    import sys
+    sys.exit(0)
+
 
 netD = Discriminator(ngpu).to(device)
 netD.apply(weights_init)
@@ -211,6 +300,8 @@ fake_label = 0
 optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))
 optimizerG = optim.Adam(netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))
 
+
+
 for epoch in range(opt.niter):
     for i, data in enumerate(dataloader, 0):
         ############################
