diff --git a/tests/perf_test.py b/tests/perf_test.py
index 363c569..9aa2d74 100644
--- a/tests/perf_test.py
+++ b/tests/perf_test.py
@@ -3,16 +3,40 @@ import torch
 from torchvision import datasets, transforms
 from torch.utils.data import DataLoader, RandomSampler
 from tqdm import tqdm
+import os
 import time
+import argparse
 
 
 def main():
-    device = 'cuda' if torch.cuda.is_available() else 'cpu'
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--ipex", action='store_true', help='use mkldnn')
+    parser.add_argument("--profile", action='store_true', help='profiling')
+    parser.add_argument("--channels_last", type=int, default=0,
+                        help='use channels last format')
+    parser.add_argument('--precision', type=str, default="float32",
+                        help='precision, float32, bfloat16')
+    # parser.add_argument('--jit', action='store_true', help='Use Pytorch jit to get boost.')
+    parser.add_argument("--batch_size", "-b", default=32, type=int)
+    parser.add_argument("--num_iter", "-n", default=0, type=int)
+    parser.add_argument("--num_warmup", "-w", default=10, type=int)
+    args = parser.parse_args()
+    print(args)
+
+    if args.ipex:
+       import intel_pytorch_extension as ipex
+       print('Running with IPEX...')
+       if args.precision == 'bfloat16':
+           ipex.enable_auto_mixed_precision(mixed_dtype=torch.bfloat16)
+           print('Running with bfloat16...')
+       device = ipex.DEVICE
+    else:
+       device = 'cuda' if torch.cuda.is_available() else 'cpu'
     print(f'Running on device "{device}"')
 
     mtcnn = MTCNN(device=device)
 
-    batch_size = 32
+    batch_size = args.batch_size
 
     # Generate data loader
     ds = datasets.ImageFolder(
@@ -21,18 +45,79 @@ def main():
     )
     dl = DataLoader(
         dataset=ds,
-        num_workers=4,
+        num_workers=1,
         collate_fn=training.collate_pil,
         batch_size=batch_size,
         sampler=RandomSampler(ds, replacement=True, num_samples=960),
     )
 
-    start = time.time()
+    # if args.jit:
+    #     # mtcnn = torch.jit.script(mtcnn)
+    #     for x, _ in dl:
+    #         mtcnn = torch.jit.trace(mtcnn, x)
+    #         break
+    i = 0
+    total_time = 0
+    total_sample = 0
     faces = []
     for x, _ in tqdm(dl):
-        faces.extend(mtcnn(x))
-    elapsed = time.time() - start
-    print(f'Elapsed: {elapsed} | EPS: {len(dl) * batch_size / elapsed}')
+        if args.num_iter > 0 and i >= args.num_iter:
+            break
+        start = time.time()
+        if args.profile:
+            with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU]) as prof:
+                if args.precision == 'bfloat16':
+                    with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                        faces.extend(mtcnn(x))
+                else:
+                    faces.extend(mtcnn(x))
+        else:
+            if args.precision == 'bfloat16':
+                with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                    faces.extend(mtcnn(x))
+            else:
+                faces.extend(mtcnn(x))
+        elapsed = time.time() - start
+        if i >= args.num_warmup:
+            total_time += elapsed
+            total_sample += batch_size
+
+        i += 1
+
+    print(f'Elapsed: {total_time} | EPS: {total_sample / total_time}')
+    print('Throughput: {}'.format(total_sample / total_time))
+
+    #
+    if args.profile:
+        import pathlib
+        timeline_dir = str(pathlib.Path.cwd()) + '/timeline/'
+        if not os.path.exists(timeline_dir):
+            os.makedirs(timeline_dir)
+        timeline_file = timeline_dir + 'timeline-' + str(torch.backends.quantized.engine) + '-' + \
+                    "facenet" + str(i) + '-' + str(os.getpid()) + '.json'
+        print(timeline_file)
+        prof.export_chrome_trace(timeline_file)
+        # table_res = prof.key_averages().table(sort_by="cpu_time_total")
+        # save_profile_result(timeline_dir + torch.backends.quantized.engine + "_result_average.xlsx", table_res)
+
+def save_profile_result(filename, table):
+    import xlsxwriter
+    workbook = xlsxwriter.Workbook(filename)
+    worksheet = workbook.add_worksheet()
+    keys = ["Name", "Self CPU total %", "Self CPU total", "CPU total %" , "CPU total", \
+            "CPU time avg", "Number of Calls"]
+    for j in range(len(keys)):
+        worksheet.write(0, j, keys[j])
+
+    lines = table.split("\n")
+    for i in range(3, len(lines)-4):
+        words = lines[i].split(" ")
+        j = 0
+        for word in words:
+            if not word == "":
+                worksheet.write(i-2, j, word)
+                j += 1
+    workbook.close()
 
 
 if __name__ == '__main__':
