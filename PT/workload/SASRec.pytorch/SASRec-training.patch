diff --git a/main.py b/main.py
index c7bb0da..17fa6f4 100644
--- a/main.py
+++ b/main.py
@@ -12,6 +12,29 @@ def str2bool(s):
         raise ValueError('Not a valid boolean string')
     return s == 'true'
 
+class AverageMeter(object):  #kyao (whole class)
+    """Computes and stores the average and current value"""
+    def __init__(self, name, fmt=':f'):
+        self.name = name
+        self.fmt = fmt
+        self.reset()
+
+    def reset(self):
+        self.val = 0
+        self.avg = 0
+        self.sum = 0
+        self.count = 0
+
+    def update(self, val, n=1):
+        self.val = val
+        self.sum += val * n
+        self.count += n
+        self.avg = self.sum / self.count
+
+    def __str__(self):
+        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'
+        return fmtstr.format(**self.__dict__)
+
 parser = argparse.ArgumentParser()
 parser.add_argument('--dataset', required=True)
 parser.add_argument('--train_dir', required=True)
@@ -20,13 +43,15 @@ parser.add_argument('--lr', default=0.001, type=float)
 parser.add_argument('--maxlen', default=50, type=int)
 parser.add_argument('--hidden_units', default=50, type=int)
 parser.add_argument('--num_blocks', default=2, type=int)
-parser.add_argument('--num_epochs', default=201, type=int)
+parser.add_argument('--num_epochs', default=21, type=int)
 parser.add_argument('--num_heads', default=1, type=int)
 parser.add_argument('--dropout_rate', default=0.5, type=float)
 parser.add_argument('--l2_emb', default=0.0, type=float)
 parser.add_argument('--device', default='cpu', type=str)
 parser.add_argument('--inference_only', default=False, type=str2bool)
 parser.add_argument('--state_dict_path', default=None, type=str)
+parser.add_argument('--bf16-train-cpu', action='store_true')
+parser.add_argument('--bf16-train-cuda', action='store_true')
 
 args = parser.parse_args()
 if not os.path.isdir(args.dataset + '_' + args.train_dir):
@@ -84,23 +109,50 @@ adam_optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.
 
 T = 0.0
 t0 = time.time()
-
+batch_time = AverageMeter('Time', ':6.3f') #kyao
 for epoch in range(epoch_start_idx, args.num_epochs + 1):
     if args.inference_only: break # just to decrease identition
+    end = time.time() #kyao
     for step in range(num_batch): # tqdm(range(num_batch), total=num_batch, ncols=70, leave=False, unit='b'):
         u, seq, pos, neg = sampler.next_batch() # tuples to ndarray
         u, seq, pos, neg = np.array(u), np.array(seq), np.array(pos), np.array(neg)
-        pos_logits, neg_logits = model(u, seq, pos, neg)
-        pos_labels, neg_labels = torch.ones(pos_logits.shape, device=args.device), torch.zeros(neg_logits.shape, device=args.device)
-        # print("\neye ball check raw_logits:"); print(pos_logits); print(neg_logits) # check pos_logits > 0, neg_logits < 0
-        adam_optimizer.zero_grad()
-        indices = np.where(pos != 0)
-        loss = bce_criterion(pos_logits[indices], pos_labels[indices])
-        loss += bce_criterion(neg_logits[indices], neg_labels[indices])
+        if args.bf16_train_cpu:
+            with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                pos_logits, neg_logits = model(u, seq, pos, neg)
+                pos_labels, neg_labels = torch.ones(pos_logits.shape, device=args.device), torch.zeros(neg_logits.shape, device=args.device)
+                # print("\neye ball check raw_logits:"); print(pos_logits); print(neg_logits) # check pos_logits > 0, neg_logits < 0
+                adam_optimizer.zero_grad()
+                indices = np.where(pos != 0)
+                loss = bce_criterion(pos_logits[indices], pos_labels[indices])
+                loss += bce_criterion(neg_logits[indices], neg_labels[indices])
+        elif args.bf16_train_cuda:
+            with torch.cuda.amp.autocast(enabled=True):
+                pos_logits, neg_logits = model(u, seq, pos, neg)
+                pos_labels, neg_labels = torch.ones(pos_logits.shape, device=args.device), torch.zeros(neg_logits.shape, device=args.device)
+                # print("\neye ball check raw_logits:"); print(pos_logits); print(neg_logits) # check pos_logits > 0, neg_logits < 0
+                adam_optimizer.zero_grad()
+                indices = np.where(pos != 0)
+                loss = bce_criterion(pos_logits[indices], pos_labels[indices])
+                loss += bce_criterion(neg_logits[indices], neg_labels[indices])
+        else:
+            pos_logits, neg_logits = model(u, seq, pos, neg)
+            pos_labels, neg_labels = torch.ones(pos_logits.shape, device=args.device), torch.zeros(neg_logits.shape, device=args.device)
+            # print("\neye ball check raw_logits:"); print(pos_logits); print(neg_logits) # check pos_logits > 0, neg_logits < 0
+            adam_optimizer.zero_grad()
+            indices = np.where(pos != 0)
+            loss = bce_criterion(pos_logits[indices], pos_labels[indices])
+            loss += bce_criterion(neg_logits[indices], neg_labels[indices])
         for param in model.item_emb.parameters(): loss += args.l2_emb * torch.norm(param)
         loss.backward()
         adam_optimizer.step()
-        print("loss in epoch {} iteration {}: {}".format(epoch, step, loss.item())) # expected 0.4~0.6 after init few epochs
+        batch_time.update(time.time() - end) #kyao
+        end = time.time() #kyao
+        ### performance computation #kyao
+        latency = batch_time.avg / args.batch_size * 1000 #kyao
+        throughput = args.batch_size / batch_time.avg #kyao
+        print('training latency: %.3f ms on %d epoch'%(latency, epoch)) #kyao
+        print('training throughput: %.3f fps on %d epoch'%(throughput, epoch)) #kyao
+        #print("loss in epoch {} iteration {}: {}".format(epoch, step, loss.item())) # expected 0.4~0.6 after init few epochs
 
     if epoch % 20 == 0:
         model.eval()
diff --git a/ml-1m_default/args.txt b/ml-1m_default/args.txt
index ac662a2..d7edcbe 100644
--- a/ml-1m_default/args.txt
+++ b/ml-1m_default/args.txt
@@ -1,14 +1,16 @@
 batch_size,128
+bf16_train_cpu,True
+bf16_train_cuda,False
 dataset,ml-1m
-device,cuda
+device,cpu
 dropout_rate,0.5
 hidden_units,50
-inference_only,True
+inference_only,False
 l2_emb,0.0
 lr,0.001
-maxlen,200
+maxlen,50
 num_blocks,2
 num_epochs,201
 num_heads,1
-state_dict_path,ml-1m_default/SASRec.epoch=601.lr=0.001.layer=2.head=1.hidden=50.maxlen=200.pth
+state_dict_path,None
 train_dir,default
\ No newline at end of file
