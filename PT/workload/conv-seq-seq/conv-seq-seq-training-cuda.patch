diff --git a/fairseq/trainer.py b/fairseq/trainer.py
index fa378ef0..f8978c0e 100644
--- a/fairseq/trainer.py
+++ b/fairseq/trainer.py
@@ -21,7 +21,6 @@ from fairseq.logging import meters, metrics
 from fairseq.nan_detector import NanDetector
 from fairseq.optim import lr_scheduler
 
-
 logger = logging.getLogger(__name__)
 
 
@@ -38,7 +37,7 @@ class Trainer(object):
     def __init__(self, args, task, model, criterion):
         self.args = args
         self.task = task
-
+        self.count = 0
         self.cuda = torch.cuda.is_available() and not args.cpu
         if self.cuda:
             self.device = torch.device('cuda')
@@ -267,8 +266,13 @@ class Trainer(object):
             epoch=epoch,
         )
 
+    
     @metrics.aggregate("train")
     def train_step(self, samples, raise_oom=False):
+        print("count inside = ", self.count)
+        self.count = self.count + 1
+        if self.count == 50:
+            exit()
         """Do forward, backward and parameter update."""
         if self._dummy_batch == "DUMMY":
             self._dummy_batch = samples[0]
@@ -283,6 +287,8 @@ class Trainer(object):
         # forward and backward pass
         logging_outputs, sample_size, ooms = [], 0, 0
         for i, sample in enumerate(samples):
+        
+                
             sample = self._prepare_sample(sample)
             if sample is None:
                 # when sample is None, run forward/backward on a dummy batch
diff --git a/fairseq_cli/train.py b/fairseq_cli/train.py
index 21771ff2..bf7bb0da 100644
--- a/fairseq_cli/train.py
+++ b/fairseq_cli/train.py
@@ -21,6 +21,7 @@ from fairseq.data import iterators
 from fairseq.logging import meters, metrics, progress_bar
 from fairseq.trainer import Trainer
 
+import time #kyao
 
 logging.basicConfig(
     format='%(asctime)s | %(levelname)s | %(name)s | %(message)s',
@@ -144,6 +145,7 @@ def should_stop_early(args, valid_loss):
 
 @metrics.aggregate('train')
 def train(args, trainer, task, epoch_itr):
+    batch_time = AverageMeter('Time', ':6.3f') #kyao
     """Train the model for one epoch."""
     # Initialize data iterator
     itr = epoch_itr.next_epoch_itr(
@@ -172,34 +174,66 @@ def train(args, trainer, task, epoch_itr):
 
     valid_subsets = args.valid_subset.split(',')
     max_update = args.max_update or math.inf
+    
+    end = time.time() #kyao
     for samples in progress:
-        with metrics.aggregate('train_inner'):
-            log_output = trainer.train_step(samples)
-            if log_output is None:  # OOM, overflow, ...
-                continue
-
-        # log mid-epoch stats
-        stats = get_training_stats(metrics.get_smoothed_values('train_inner'))
-        num_updates = trainer.get_num_updates()
-        progress.log(stats, tag='train_inner', step=num_updates)
-
-        # reset mid-epoch stats after each log interval
-        # the end-of-epoch stats will still be preserved
-        if num_updates % args.log_interval == 0:
-            metrics.reset_meters('train_inner')
-
-        if (
-            not args.disable_validation
-            and args.save_interval_updates > 0
-            and num_updates % args.save_interval_updates == 0
-            and num_updates > 0
-        ):
-            valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
-            checkpoint_utils.save_checkpoint(args, trainer, epoch_itr, valid_losses[0])
+        with torch.profiler.profile(
+            activities = [torch.profiler.ProfilerActivity.CPU]
+        ) as prof:
+            with metrics.aggregate('train_inner'):
+                with torch.cuda.amp.autocast(enabled=True):
+                    print('is training with bf16 on cpu...')
+                    log_output = trainer.train_step(samples)
+                    if log_output is None:  # OOM, overflow, ...
+                        continue
+
+            # log mid-epoch stats
+            stats = get_training_stats(metrics.get_smoothed_values('train_inner'))
+            num_updates = trainer.get_num_updates()
+            progress.log(stats, tag='train_inner', step=num_updates)
+
+            # reset mid-epoch stats after each log interval
+            # the end-of-epoch stats will still be preserved
+            if num_updates % args.log_interval == 0:
+                metrics.reset_meters('train_inner')
+
+            if (
+                not args.disable_validation
+                and args.save_interval_updates > 0
+                and num_updates % args.save_interval_updates == 0
+                and num_updates > 0
+            ):
+                with torch.cuda.amp.autocast(enabled=True):
+                    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
+                checkpoint_utils.save_checkpoint(args, trainer, epoch_itr, valid_losses[0])
+            
+        import os
+        import pathlib
+        timeline_dir = str(pathlib.Path.cwd()) + '/timeline/'
+        if not os.path.exists(timeline_dir):
+            os.makedirs(timeline_dir)
+        timeline_file = timeline_dir + 'timeline-' + str(torch.backends.quantized.engine) + '-' + \
+                    str(1) + '-' + str(os.getpid()) + '.json'
+
+        prof.export_chrome_trace(timeline_file)
+        
+        batch_time.update(time.time() - end) #kyao
 
         if num_updates >= max_update:
             break
-
+        
+        ### performance computation #kyao
+        latency = batch_time.avg / args.max_sentences * 1000 #kyao
+        throughput = args.max_sentences / batch_time.avg #kyao
+        print('training latency: %3.0f ms on %d epoch'%(latency, epoch_itr.epoch)) #kyao
+        print('training throughput: %3.0f fps on %d epoch'%(throughput, epoch_itr.epoch)) #kyao
+    
+    ### performance computation #kyao
+    latency = batch_time.avg / args.max_sentences * 1000 #kyao
+    throughput = args.max_sentences / batch_time.avg #kyao
+    print('training latency: %3.0f ms on %d epoch'%(latency, epoch_itr.epoch)) #kyao
+    print('training throughput: %3.0f fps on %d epoch'%(throughput, epoch_itr.epoch)) #kyao
+    
     # log end-of-epoch stats
     stats = get_training_stats(metrics.get_smoothed_values('train'))
     progress.print(stats, tag='train', step=num_updates)
@@ -321,6 +355,28 @@ def cli_main(modify_parser=None):
         # single GPU training
         main(args)
 
+class AverageMeter(object):  #kyao (whole class)
+    """Computes and stores the average and current value"""
+    def __init__(self, name, fmt=':f'):
+        self.name = name
+        self.fmt = fmt
+        self.reset()
+
+    def reset(self):
+        self.val = 0
+        self.avg = 0
+        self.sum = 0
+        self.count = 0
+
+    def update(self, val, n=1):
+        self.val = val
+        self.sum += val * n
+        self.count += n
+        self.avg = self.sum / self.count
+
+    def __str__(self):
+        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'
+        return fmtstr.format(**self.__dict__)
 
 if __name__ == '__main__':
     cli_main()
