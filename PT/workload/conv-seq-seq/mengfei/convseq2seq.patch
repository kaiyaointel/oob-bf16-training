diff --git a/fairseq/search.py b/fairseq/search.py
index 4e93d3f0..8f535d21 100644
--- a/fairseq/search.py
+++ b/fairseq/search.py
@@ -75,7 +75,8 @@ class BeamSearch(Search):
         )
         scores_buf = top_prediction[0]
         indices_buf = top_prediction[1]
-        beams_buf = torch.div(indices_buf, vocab_size)
+        # beams_buf = torch.div(indices_buf, vocab_size)
+        beams_buf = indices_buf // vocab_size
         indices_buf.fmod_(vocab_size)
         return scores_buf, indices_buf, beams_buf
 
diff --git a/fairseq_cli/generate.py b/fairseq_cli/generate.py
index 61c41e35..5bea136d 100644
--- a/fairseq_cli/generate.py
+++ b/fairseq_cli/generate.py
@@ -19,6 +19,26 @@ from fairseq.logging import progress_bar
 from fairseq.logging.meters import StopwatchMeter, TimeMeter
 
 
+def save_profile_result(filename, table):
+    import xlsxwriter
+    workbook = xlsxwriter.Workbook(filename)
+    worksheet = workbook.add_worksheet()
+    keys = ["Name", "Self CPU total %", "Self CPU total", "CPU total %" , "CPU total", \
+            "CPU time avg", "Number of Calls"]
+    for j in range(len(keys)):
+        worksheet.write(0, j, keys[j])
+
+    lines = table.split("\n")
+    for i in range(3, len(lines)-4):
+        words = lines[i].split(" ")
+        j = 0
+        for word in words:
+            if not word == "":
+                worksheet.write(i-2, j, word)
+                j += 1
+    workbook.close()
+
+
 def main(args):
     assert args.path is not None, '--path required for generation!'
     assert not args.sampling or args.nbest == args.beam, \
@@ -81,6 +101,14 @@ def _main(args, output_file):
             model.half()
         if use_cuda:
             model.cuda()
+        if args.ipex:
+            import intel_pytorch_extension as ipex
+            model.to(ipex.DEVICE)
+        if args.channels_last:
+            model_oob = model
+            model_oob = model_oob.to(memory_format=torch.channels_last)
+            model = model_oob
+            print("---- Use channels last format.")
 
     # Load alignment dictionary for unknown word replacement
     # (None if no unknown word replacement, empty if no path to align dictionary)
@@ -120,7 +148,9 @@ def _main(args, output_file):
     num_sentences = 0
     has_target = True
     wps_meter = TimeMeter()
+    samples_num = 0
     for sample in progress:
+        samples_num = samples_num + 1
         sample = utils.move_to_cuda(sample) if use_cuda else sample
         if 'net_input' not in sample:
             continue
@@ -130,8 +160,23 @@ def _main(args, output_file):
             prefix_tokens = sample['target'][:, :args.prefix_size]
 
         gen_timer.start()
-        hypos = task.inference_step(generator, models, sample, prefix_tokens)
-        num_generated_tokens = sum(len(h[0]['tokens']) for h in hypos)
+        if args.profile:
+            with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU]) as prof:
+                if args.precision == "bfloat16":
+                    with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                        hypos = task.inference_step(generator, models, sample, prefix_tokens)
+                        num_generated_tokens = sum(len(h[0]['tokens']) for h in hypos)
+                else:
+                    hypos = task.inference_step(generator, models, sample, prefix_tokens)
+                    num_generated_tokens = sum(len(h[0]['tokens']) for h in hypos)
+        else:
+            if args.precision == "bfloat16":
+                with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
+                    hypos = task.inference_step(generator, models, sample, prefix_tokens)
+                    num_generated_tokens = sum(len(h[0]['tokens']) for h in hypos)
+            else:
+                hypos = task.inference_step(generator, models, sample, prefix_tokens)
+                num_generated_tokens = sum(len(h[0]['tokens']) for h in hypos)
         gen_timer.stop(num_generated_tokens)
 
         for i, sample_id in enumerate(sample['id'].tolist()):
@@ -157,9 +202,9 @@ def _main(args, output_file):
 
             if not args.quiet:
                 if src_dict is not None:
-                    print('S-{}\t{}'.format(sample_id, src_str), file=output_file)
+                    print('S-{}\t{}'.format(sample_id, src_str).encode('utf-8'), file=output_file)
                 if has_target:
-                    print('T-{}\t{}'.format(sample_id, target_str), file=output_file)
+                    print('T-{}\t{}'.format(sample_id, target_str).encode('utf-8'), file=output_file)
 
             # Process top predictions
             for j, hypo in enumerate(hypos[i][:args.nbest]):
@@ -174,7 +219,7 @@ def _main(args, output_file):
 
                 if not args.quiet:
                     score = hypo['score'] / math.log(2)  # convert to base 2
-                    print('H-{}\t{}\t{}'.format(sample_id, score, hypo_str), file=output_file)
+                    print('H-{}\t{}\t{}'.format(sample_id, score, hypo_str).encode('utf-8'), file=output_file)
                     print('P-{}\t{}'.format(
                         sample_id,
                         ' '.join(map(
@@ -218,10 +263,26 @@ def _main(args, output_file):
         wps_meter.update(num_generated_tokens)
         progress.log({'wps': round(wps_meter.avg)})
         num_sentences += sample['nsentences']
+        if args.iteration != 0 and samples_num > args.iteration:
+            break
+    #
+    if args.profile:
+        import pathlib
+        timeline_dir = str(pathlib.Path.cwd()) + '/timeline/'
+        if not os.path.exists(timeline_dir):
+            os.makedirs(timeline_dir)
+        timeline_file = timeline_dir + 'timeline-' + str(torch.backends.quantized.engine) + '-' + \
+                    args.arch + str(samples_num) + '-' + str(os.getpid()) + '.json'
+        print(timeline_file)
+        prof.export_chrome_trace(timeline_file)
+        # table_res = prof.key_averages().table(sort_by="cpu_time_total")
+        # self.save_profile_result(timeline_dir + torch.backends.quantized.engine + "_result_average.xlsx", table_res)
 
     logger.info('NOTE: hypothesis and token scores are output in base 2')
     logger.info('Translated {} sentences ({} tokens) in {:.1f}s ({:.2f} sentences/s, {:.2f} tokens/s)'.format(
         num_sentences, gen_timer.n, gen_timer.sum, num_sentences / gen_timer.sum, 1. / gen_timer.avg))
+    logger.info('Throughput: ({:.2f} sentences/s'.format(num_sentences / gen_timer.sum))
+
     if has_target:
         logger.info('Generate {} with beam={}: {}'.format(args.gen_subset, args.beam, scorer.result_string()))
 
@@ -230,6 +291,12 @@ def _main(args, output_file):
 
 def cli_main():
     parser = options.get_generation_parser()
+    parser.add_argument('--ipex', action='store_true', help='use ipex')
+    parser.add_argument('--iteration', '-i', type=int, default=0, help='samples iteration')
+    parser.add_argument('--channels_last', type=int, default=1, help='use channels last format')
+    parser.add_argument('--precision', type=str, default='float32', help='float32, bfloat16')
+    parser.add_argument('--arch', type=str, default='convseq2seq', help='model name')
+    parser.add_argument('--profile', action='store_true', help='Trigger profile on current topology.')
     args = options.parse_args_and_arch(parser)
     main(args)
 
